import asyncio
import time
from collections import defaultdict
from datetime import datetime, timedelta
from loguru import logger
from typing import List
from sqlalchemy.orm import Session
from sqlalchemy.orm.attributes import flag_modified
from sqlalchemy.exc import IntegrityError
from app.core.config import settings
from app.core.database import SessionLocal
from app.core.utils import (
    is_empty_value,
    not_empty,
    get_ai_solution,
    get_soothing_reply,
    get_similar_cause,
    get_similar_solution,
    DEFAULT_AI_SOLUTION,
    DEFAULT_SOOTHING_REPLY,
    DEFAULT_AI_SOLUTIONS,
)
from app.models.chat_record import ChatRecord
from app.models.sql_models import (
    WeComMessage,
    Issue,
    TicketDraft,
    IngestState,
    RoomAssignee,
    AlertEvent,
    RoomInfo,
    RoomPollingState,
)
from app.services.data_service import _extract_content
from app.services.data_clean_service import DataCleanService
from app.services.ticket_service import (
    build_ticket_draft,
    build_ticket_markdown,
    build_ticket_title,
    build_tb_note,
    build_customfields_pending,
    build_customfields_for_create,
    build_ai_assistant_text,
    build_tb_ai_assistant_text,
    normalize_issue_type,
    normalize_priority,
    markdown_to_plain_text,
    generate_ticket_title_llm,
    generate_note_summary_llm,
    extract_versions_and_image_llm,
    analyze_complete_llm,
    pre_judge_has_issue,
)
from app.services.reply_service import generate_reply
from app.services import data_service
from app.services.vector_service import vector_kb
from app.agents.assistant import AssistantAgent
from app.services.wecom_service import WeComService
from app.agents.sentinel import SentinelAgent
from app.services.dingtalk_service import DingTalkService, risk_score_to_priority
from app.services.alert_policy_service import should_send_alert, build_aggregate_summary
from app.services.aggregation_service import update_issue_aggregation
from app.services.faq_service import FaqService
from app.services.issue_filter_service import is_hard_issue, check_resolved_status
from app.services.teambition_service import create_task, get_task_url, build_task_payload
from app.services.mcp_bridge_service import submit_mcp_task
from app.services.teambition_oapi_service import create_task_oapi, update_task_customfield


SOURCE_KEY = "chat_records"
ARCHIVE_SOURCE_KEY = "wecom_archive"

# ============ æ‰¹å¤„ç†æ¨¡å¼ï¼šå†·å´æ—¶é—´è·Ÿè¸ª ============
# è®°å½•æ¯ä¸ª room_id ä¸Šæ¬¡å®Œæ•´å¤„ç†çš„æ—¶é—´æˆ³ï¼ˆå†…å­˜å­—å…¸ï¼ŒæœåŠ¡é‡å¯ä¼šé‡ç½®ï¼‰
_room_last_full_process: dict[str, float] = {}

# ============ æ’é™¤ç¾¤åˆ—è¡¨ç¼“å­˜ï¼ˆé¿å…æ¯è½®å…¨è¡¨æ‰«æï¼‰ ============
_excluded_rooms_cache: set[str] = set()
_excluded_rooms_cache_ts: float = 0

# ============ ç¾¤èŠç»´åº¦è½®è¯¢ï¼šæ¯ä¸ªç¾¤çš„çŠ¶æ€ ============
# ç»“æ„: { "room_id": {"last_msgtime": int, "pending_count": int, "last_processed_at": float} }
_room_state: dict[str, dict] = {}

# ============ æ¯æ—¥å‘¨æœŸè·Ÿè¸ª ============
# å½“å‰å‘¨æœŸæ—¥æœŸï¼ˆæ ¼å¼ YYYY-MM-DDï¼‰ï¼Œç”¨äºæ£€æµ‹æ˜¯å¦è·¨è¶Š9:00è¿›å…¥æ–°å‘¨æœŸ
_current_cycle_date: str = ""


def _get_current_cycle_start() -> int:
    """
    è·å–å½“å‰å‘¨æœŸçš„èµ·å§‹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
    
    å‘¨æœŸå®šä¹‰ï¼š
    - å¦‚æœå½“å‰æ—¶é—´ >= å½“æ—¥ DAILY_CYCLE_START_HOUR ç‚¹ï¼Œè¿”å›ä»Šå¤©è¯¥æ—¶åˆ»
    - å¦‚æœå½“å‰æ—¶é—´ < å½“æ—¥ DAILY_CYCLE_START_HOUR ç‚¹ï¼Œè¿”å›æ˜¨å¤©è¯¥æ—¶åˆ»
    
    ä¾‹å¦‚ DAILY_CYCLE_START_HOUR=9ï¼š
    - å½“å‰ 2/5 14:00 -> è¿”å› 2/5 09:00
    - å½“å‰ 2/5 08:00 -> è¿”å› 2/4 09:00
    
    Returns:
        å‘¨æœŸèµ·å§‹æ—¶é—´çš„æ¯«ç§’æ—¶é—´æˆ³
    """
    now = datetime.now()
    cycle_hour = settings.DAILY_CYCLE_START_HOUR
    today_cycle_start = now.replace(hour=cycle_hour, minute=0, second=0, microsecond=0)
    
    if now >= today_cycle_start:
        cycle_start = today_cycle_start
    else:
        cycle_start = today_cycle_start - timedelta(days=1)
    
    return int(cycle_start.timestamp() * 1000)


def _check_and_reset_cycle(db: Session = None) -> bool:
    """
    æ£€æµ‹æ˜¯å¦è¿›å…¥æ–°çš„æ¯æ—¥å‘¨æœŸï¼Œå¦‚æœæ˜¯åˆ™é‡ç½®æ‰€æœ‰ç¾¤èŠçŠ¶æ€
    
    æ¯å¤© DAILY_CYCLE_START_HOUR ç‚¹ï¼ˆé»˜è®¤9:00ï¼‰å¼€å§‹æ–°å‘¨æœŸï¼š
    - é‡ç½®æ‰€æœ‰ç¾¤çš„ last_msgtime åˆ°æ–°å‘¨æœŸèµ·ç‚¹
    - é‡ç½®æ‰€æœ‰ç¾¤çš„ pending_count ä¸º 0
    - å¦‚æœæä¾›äº† db å‚æ•°ï¼Œä¼šå°†é‡ç½®åçš„çŠ¶æ€æŒä¹…åŒ–åˆ°æ•°æ®åº“
    
    Args:
        db: æ•°æ®åº“ä¼šè¯ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºæŒä¹…åŒ–é‡ç½®åçš„çŠ¶æ€
    
    Returns:
        True å¦‚æœå‘ç”Ÿäº†å‘¨æœŸé‡ç½®ï¼Œå¦åˆ™ False
    """
    global _current_cycle_date, _room_state
    
    cycle_start_ms = _get_current_cycle_start()
    cycle_date = datetime.fromtimestamp(cycle_start_ms / 1000).strftime("%Y-%m-%d")
    
    if _current_cycle_date != cycle_date:
        old_cycle = _current_cycle_date or "(é¦–æ¬¡å¯åŠ¨)"
        
        # é‡ç½®å‰ï¼šæ ‡è®°æœ‰æœªåˆ†ææ¶ˆæ¯çš„ç¾¤èŠä¸º needs_flushï¼Œé¿å…æ¶ˆæ¯ä¸¢å¤±
        flush_rooms = []
        for room_id, state in _room_state.items():
            p = state.get("pending_count", 0)
            r = state.get("raw_pending_count", 0)
            if p >= 2 or r >= 5:
                state["needs_flush"] = True
                flush_rooms.append((room_id, p, r))
        
        if flush_rooms:
            logger.info(
                f"[å‘¨æœŸé‡ç½®] å‘ç° {len(flush_rooms)} ä¸ªç¾¤æœ‰æœªåˆ†ææ¶ˆæ¯ï¼Œå·²æ ‡è®°ä¸º needs_flushï¼Œ"
                f"å°†åœ¨ä¸‹ä¸€è½®ä¼˜å…ˆåˆ†æåå†æ¸…é›¶"
            )
            for rid, p, r in flush_rooms:
                logger.debug(f"  needs_flush: room={rid}, pending={p}, raw={r}")
        
        logger.info(
            f"[å‘¨æœŸé‡ç½®] è¿›å…¥æ–°å‘¨æœŸ: {cycle_date}ï¼ˆä¸Šä¸€å‘¨æœŸ: {old_cycle}ï¼‰ï¼Œ"
            f"é‡ç½® {len(_room_state)} ä¸ªç¾¤èŠçŠ¶æ€ï¼Œå‘¨æœŸèµ·ç‚¹={datetime.fromtimestamp(cycle_start_ms/1000)}"
        )
        _current_cycle_date = cycle_date
        
        # é‡ç½®æ‰€æœ‰ç¾¤çš„æ¸¸æ ‡åˆ°æ–°å‘¨æœŸèµ·ç‚¹
        # æ³¨æ„ï¼šneeds_flush çš„ç¾¤ä¿ç•™ pending_countï¼Œç­‰åˆ†æå®Œå†æ¸…é›¶
        for room_id in _room_state:
            _room_state[room_id]["last_msgtime"] = cycle_start_ms
            if not _room_state[room_id].get("needs_flush"):
                _room_state[room_id]["pending_count"] = 0
                _room_state[room_id]["raw_pending_count"] = 0
        
        # æŒä¹…åŒ–é‡ç½®åçš„çŠ¶æ€åˆ°æ•°æ®åº“
        if db and _room_state:
            _save_all_room_states(db)
        
        return True
    return False


def _get_room_state(room_id: str) -> dict:
    """è·å–æˆ–åˆå§‹åŒ–ç¾¤èŠçŠ¶æ€"""
    if room_id not in _room_state:
        _room_state[room_id] = {
            "last_msgtime": 0,          # è¯¥ç¾¤å·²å¤„ç†åˆ°çš„æ¸¸æ ‡
            "pending_count": 0,          # ç´¯ç§¯æœªåˆ†æçš„æœ‰æ•ˆæ¶ˆæ¯æ•°ï¼ˆéå™ªéŸ³ï¼‰
            "raw_pending_count": 0,      # ç´¯ç§¯æœªåˆ†æçš„åŸå§‹æ¶ˆæ¯æ•°ï¼ˆæ‰€æœ‰ textï¼‰
            "last_processed_at": 0,      # ä¸Šæ¬¡å¤„ç†æ—¶é—´ï¼ˆç”¨äºæ’åºï¼‰
        }
    return _room_state[room_id]


def _load_all_room_states(db: Session) -> int:
    """
    å¯åŠ¨æ—¶ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰ç¾¤èŠçŠ¶æ€åˆ°å†…å­˜
    
    åŒæ—¶è®¾ç½®å½“å‰å‘¨æœŸæ—¥æœŸï¼Œé¿å…è¯¯è§¦å‘å‘¨æœŸé‡ç½®
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
    
    Returns:
        åŠ è½½çš„çŠ¶æ€æ•°é‡
    """
    global _room_state, _current_cycle_date
    
    try:
        states = db.query(RoomPollingState).all()
        for state in states:
            _room_state[state.room_id] = {
                "last_msgtime": int(state.last_msgtime or 0),
                "pending_count": int(state.pending_count or 0),
                "raw_pending_count": int(state.raw_pending_count or 0),
                "last_processed_at": float(state.last_processed_at or 0) / 1000,  # è½¬æ¢ä¸ºç§’
            }
        
        # åŠ è½½çŠ¶æ€åï¼Œè®¾ç½®å½“å‰å‘¨æœŸæ—¥æœŸï¼Œé¿å… _check_and_reset_cycle è¯¯è§¦å‘é‡ç½®
        # è¿™æ ·åªæœ‰çœŸæ­£è·¨å¤©ï¼ˆè¿‡äº†9:00ï¼‰æ—¶æ‰ä¼šé‡ç½®
        if states:
            cycle_start_ms = _get_current_cycle_start()
            _current_cycle_date = datetime.fromtimestamp(cycle_start_ms / 1000).strftime("%Y-%m-%d")
            logger.info(f"[çŠ¶æ€åŠ è½½] ä»æ•°æ®åº“åŠ è½½äº† {len(states)} ä¸ªç¾¤èŠçŠ¶æ€ï¼Œå½“å‰å‘¨æœŸ={_current_cycle_date}")
        else:
            logger.info("[çŠ¶æ€åŠ è½½] æ•°æ®åº“æ— å†å²çŠ¶æ€ï¼Œå°†ä»å¤´å¼€å§‹")
        
        return len(states)
    except Exception as e:
        logger.warning(f"[çŠ¶æ€åŠ è½½] åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä»å¤´å¼€å§‹")
        return 0


def _save_room_state(db: Session, room_id: str) -> None:
    """
    ä¿å­˜å•ä¸ªç¾¤èŠçŠ¶æ€åˆ°æ•°æ®åº“
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        room_id: ç¾¤èŠID
    """
    state = _room_state.get(room_id)
    if not state:
        return
    
    try:
        db_state = db.query(RoomPollingState).filter(
            RoomPollingState.room_id == room_id
        ).first()
        
        if db_state:
            db_state.last_msgtime = state["last_msgtime"]
            db_state.pending_count = state["pending_count"]
            db_state.raw_pending_count = state.get("raw_pending_count", 0)
            db_state.last_processed_at = int(state["last_processed_at"] * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        else:
            db_state = RoomPollingState(
                room_id=room_id,
                last_msgtime=state["last_msgtime"],
                pending_count=state["pending_count"],
                raw_pending_count=state.get("raw_pending_count", 0),
                last_processed_at=int(state["last_processed_at"] * 1000),
            )
            db.add(db_state)
        
        db.commit()
    except Exception as e:
        logger.warning(f"[çŠ¶æ€ä¿å­˜] ä¿å­˜ room={room_id} çŠ¶æ€å¤±è´¥: {e}")
        db.rollback()


def _save_all_room_states(db: Session) -> None:
    """
    ä¿å­˜æ‰€æœ‰ç¾¤èŠçŠ¶æ€åˆ°æ•°æ®åº“ï¼ˆç”¨äºå‘¨æœŸé‡ç½®æ—¶æ‰¹é‡ä¿å­˜ï¼‰
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
    """
    saved_count = 0
    for room_id in _room_state:
        _save_room_state(db, room_id)
        saved_count += 1
    
    logger.info(f"[çŠ¶æ€ä¿å­˜] æ‰¹é‡ä¿å­˜äº† {saved_count} ä¸ªç¾¤èŠçŠ¶æ€")


def _is_in_cooldown(room_id: str) -> bool:
    """æ£€æŸ¥ room_id æ˜¯å¦åœ¨å†·å´æœŸå†…"""
    last_time = _room_last_full_process.get(room_id, 0)
    return (time.time() - last_time) < settings.ROOM_COOLDOWN_SECONDS


def _update_cooldown(room_id: str) -> None:
    """æ›´æ–° room_id çš„å†·å´æ—¶é—´"""
    _room_last_full_process[room_id] = time.time()


async def _ai_dedup_check(new_phenomenon: str, existing_phenomena: list[str]) -> bool:
    """
    AI è¯­ä¹‰å»é‡å…œåº•ï¼šåˆ¤æ–­æ–°é—®é¢˜æ˜¯å¦ä¸å·²æœ‰é—®é¢˜åˆ—è¡¨ä¸­çš„æŸä¸ªé—®é¢˜æœ¬è´¨ç›¸åŒã€‚
    ä»…åœ¨ç®—æ³•å±‚æœªå‘½ä¸­ã€ä¸”åŒç¾¤å·²æœ‰å·¥å•æ—¶è°ƒç”¨ã€‚
    
    Args:
        new_phenomenon: æ–°é—®é¢˜çš„ç°è±¡æè¿°
        existing_phenomena: å·²æœ‰é—®é¢˜çš„ç°è±¡æè¿°åˆ—è¡¨
    
    Returns:
        True å¦‚æœ AI åˆ¤å®šä¸ºé‡å¤é—®é¢˜
    """
    from app.core.llm_factory import get_fast_llm
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    
    if not existing_phenomena:
        return False
    
    existing_list = "\n".join(f"- {p}" for p in existing_phenomena if p)
    if not existing_list:
        return False
    
    prompt_text = (
        "åˆ¤æ–­ã€æ–°é—®é¢˜ã€‘æ˜¯å¦ä¸ã€å·²æœ‰é—®é¢˜ã€‘åˆ—è¡¨ä¸­çš„æŸä¸ªé—®é¢˜æ˜¯åŒä¸€ä¸ªé—®é¢˜ï¼ˆä¸åŒè¡¨è¿°ä½†æœ¬è´¨ç›¸åŒï¼‰ã€‚\n\n"
        f"ã€æ–°é—®é¢˜ã€‘ï¼š{new_phenomenon}\n\n"
        f"ã€å·²æœ‰é—®é¢˜ã€‘ï¼š\n{existing_list}\n\n"
        "åªè¾“å‡º YES æˆ– NOã€‚YES=åŒä¸€é—®é¢˜çš„ä¸åŒè¡¨è¿°ï¼ŒNO=å®Œå…¨ä¸åŒçš„é—®é¢˜ã€‚"
    )
    
    try:
        prompt = ChatPromptTemplate.from_template("{text}")
        chain = prompt | get_fast_llm() | StrOutputParser()
        result = await chain.ainvoke({"text": prompt_text})
        answer = (result or "").strip().upper()
        is_dup = answer.startswith("YES")
        logger.debug(f"[AIå»é‡] æ–°='{new_phenomenon[:20]}', å·²æœ‰={len(existing_phenomena)}ä¸ª, AIå›ç­”={answer}, åˆ¤å®š={'é‡å¤' if is_dup else 'ä¸åŒ'}")
        return is_dup
    except Exception as e:
        logger.warning(f"[AIå»é‡] LLMè°ƒç”¨å¤±è´¥: {e}ï¼Œé»˜è®¤æ”¾è¡Œ")
        return False


async def _is_duplicate_issue(db: Session, room_id: str, phenomenon: str, cycle_start_ms: int) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼é—®é¢˜çš„å·¥å•ï¼ˆæ”¯æŒå…¨å±€å»é‡ï¼‰
    
    å»é‡é€»è¾‘ï¼š
    - å¦‚æœ ISSUE_DEDUP_GLOBAL=True: å…¨å±€æŸ¥è¯¢ï¼ˆè·¨ç¾¤èŠï¼‰ï¼Œæ—¶é—´çª—å£ä¸º ISSUE_DEDUP_DAYS å¤©
    - å¦‚æœ ISSUE_DEDUP_GLOBAL=False: ä»…åŒ room_id + å½“å‰å‘¨æœŸï¼ˆåŸé€»è¾‘ï¼‰
    - phenomenonï¼ˆé—®é¢˜ç°è±¡ï¼‰ç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼ˆåŸºäºå…³é”®è¯é‡å ï¼‰
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        room_id: ç¾¤èŠID
        phenomenon: æ–°é—®é¢˜çš„ç°è±¡æè¿°
        cycle_start_ms: å½“å‰å‘¨æœŸèµ·å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    
    Returns:
        True å¦‚æœå‘ç°é‡å¤é—®é¢˜ï¼Œåº”è·³è¿‡å»ºå•ï¼›False è¡¨ç¤ºæ˜¯æ–°é—®é¢˜
    """
    if not phenomenon:
        return False
    
    # æ ¹æ®é…ç½®å†³å®šæŸ¥è¯¢èŒƒå›´
    if settings.ISSUE_DEDUP_GLOBAL:
        # å…¨å±€å»é‡ï¼šè·¨ç¾¤èŠï¼Œä½¿ç”¨å¯é…ç½®çš„å¤©æ•°çª—å£
        since_dt = datetime.now() - timedelta(days=settings.ISSUE_DEDUP_DAYS)
        existing_tickets = (
            db.query(TicketDraft)
            .filter(TicketDraft.created_at >= since_dt)
            .all()
        )
        dedup_scope = f"å…¨å±€({settings.ISSUE_DEDUP_DAYS}å¤©)"
    else:
        # åŸé€»è¾‘ï¼šä»…åŒ room_id + å½“å‰å‘¨æœŸ
        cycle_start_dt = datetime.fromtimestamp(cycle_start_ms / 1000)
        existing_tickets = (
            db.query(TicketDraft)
            .filter(
                TicketDraft.room_id == room_id,
                TicketDraft.created_at >= cycle_start_dt,
            )
            .all()
        )
        dedup_scope = "åŒç¾¤+å½“å‰å‘¨æœŸ"
    
    if not existing_tickets:
        return False
    
    # æå–å…³é”®è¯ï¼šä½¿ç”¨å­—ç¬¦çº§ bigramï¼ˆ2-gramï¼‰æ”¯æŒä¸­æ–‡å»é‡
    # ä¾‹å¦‚ "äº‘æœºæ— æ³•å¼€æœº" â†’ {"äº‘æœº", "æœºæ— ", "æ— æ³•", "æ³•å¼€", "å¼€æœº"}
    def _extract_bigrams(text: str) -> set:
        """æå–ä¸­æ–‡ bigram é›†åˆï¼ŒåŒæ—¶ä¿ç•™è‹±æ–‡å•è¯"""
        import re
        text = text.lower().strip()
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[ï¼Œã€‚ã€ï¼ï¼Ÿï¼šï¼›""''ï¼ˆï¼‰\(\)\[\]\s]+', '', text)
        if len(text) < 2:
            return set()
        # æå–æ‰€æœ‰è¿ç»­2å­—ç¬¦çš„ç»„åˆ
        bigrams = {text[i:i+2] for i in range(len(text) - 1)}
        return bigrams
    
    new_bigrams = _extract_bigrams(phenomenon)
    
    if not new_bigrams:
        return False
    
    threshold = settings.ISSUE_DEDUP_SIMILARITY_THRESHOLD
    
    for ticket in existing_tickets:
        # ä» content JSON ä¸­æå– phenomenon
        old_phenomenon = ""
        if ticket.content and isinstance(ticket.content, dict):
            old_phenomenon = ticket.content.get("phenomenon", "")
        
        if not old_phenomenon:
            continue
        
        old_bigrams = _extract_bigrams(old_phenomenon)
        
        if not old_bigrams:
            continue
        
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼šJaccard + containment å–æœ€å¤§å€¼
        # containment è§£å†³é•¿çŸ­æ–‡æœ¬ç¨€é‡Šé—®é¢˜ï¼ˆçŸ­æ–‡æœ¬å¤§éƒ¨åˆ† bigram è¢«é•¿æ–‡æœ¬åŒ…å«å³è§†ä¸ºé‡å¤ï¼‰
        overlap = len(new_bigrams & old_bigrams)
        union = len(new_bigrams | old_bigrams)
        min_len = min(len(new_bigrams), len(old_bigrams))
        jaccard = overlap / union if union > 0 else 0
        containment = overlap / min_len if min_len > 0 else 0
        similarity = max(jaccard, containment)
        
        if similarity >= threshold:
            # å¢å¼ºæ—¥å¿—ï¼šæ˜¾ç¤ºé¦–æ¬¡å‡ºç°çš„ç¾¤èŠå’Œæ—¶é—´
            first_room = ticket.room_id or "æœªçŸ¥"
            first_time = ticket.created_at.strftime("%Y-%m-%d %H:%M") if ticket.created_at else "æœªçŸ¥"
            logger.info(
                f"[å»é‡å‘½ä¸­] èŒƒå›´={dedup_scope}, å½“å‰ç¾¤={room_id}, "
                f"é¦–æ¬¡å‡ºç°ç¾¤={first_room}, é¦–æ¬¡æ—¶é—´={first_time}, "
                f"ç›¸ä¼¼åº¦={similarity:.2f}, é˜ˆå€¼={threshold}, "
                f"æ–°é—®é¢˜='{phenomenon[:30]}...', å·²æœ‰='{old_phenomenon[:30]}...'"
            )
            return True
    
    # ========== AI è¯­ä¹‰å»é‡å…œåº• ==========
    # ç®—æ³•å±‚æœªå‘½ä¸­ï¼Œä½†å¦‚æœåŒç¾¤å·²æœ‰å·¥å•ï¼Œè°ƒç”¨ LLM åšæœ€ç»ˆç¡®è®¤
    same_room_tickets = [t for t in existing_tickets if t.room_id == room_id]
    if same_room_tickets:
        existing_phenomena = []
        for t in same_room_tickets:
            if t.content and isinstance(t.content, dict):
                p = t.content.get("phenomenon", "")
                if p:
                    existing_phenomena.append(p)
        
        if existing_phenomena:
            is_dup = await _ai_dedup_check(phenomenon, existing_phenomena)
            if is_dup:
                logger.info(
                    f"[AIå»é‡å‘½ä¸­] èŒƒå›´={dedup_scope}, room={room_id}, "
                    f"æ–°é—®é¢˜='{phenomenon[:30]}', AIåˆ¤å®šä¸å·²æœ‰å·¥å•é‡å¤"
                )
                return True
    
    return False


def _format_issue_time(msgtime_ms: int | None) -> str | None:
    """
    å°†æ¯«ç§’æ—¶é—´æˆ³æ ¼å¼åŒ–ä¸º 'æœˆ/æ—¥ æ—¶:åˆ†' æ ¼å¼
    
    Args:
        msgtime_ms: æ¯«ç§’æ—¶é—´æˆ³
    
    Returns:
        æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²ï¼Œå¦‚ '2/3 09:22'
    """
    if not msgtime_ms:
        return None
    from datetime import datetime
    try:
        dt = datetime.fromtimestamp(msgtime_ms / 1000)
        # Windows ä½¿ç”¨ %#m/%#dï¼ŒLinux ä½¿ç”¨ %-m/%-d
        # ä¸ºå…¼å®¹æ€§ï¼Œä½¿ç”¨ lstrip('0') æ‰‹åŠ¨å»é™¤å‰å¯¼é›¶
        month = str(dt.month)
        day = str(dt.day)
        time_str = dt.strftime("%H:%M")
        return f"{month}/{day} {time_str}"
    except Exception:
        return None


# ============ æ‰¹å¤„ç†æ¨¡å¼ï¼šé«˜é£é™©å…³é”®è¯æ£€æµ‹ ============
def _contains_high_risk_keyword(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«é«˜é£é™©å…³é”®è¯ï¼Œå‘½ä¸­åˆ™ç»•è¿‡å†·å´ç«‹å³å¤„ç†"""
    if not text:
        return False
    keywords = [k.strip() for k in settings.HIGH_RISK_KEYWORDS.split(",") if k.strip()]
    text_lower = text.lower()
    return any(kw in text_lower for kw in keywords)


# ============ æ‰¹å¤„ç†æ¨¡å¼ï¼šæŒ‰ room_id èšåˆæ¶ˆæ¯ ============
def _group_messages_by_room(records: list) -> dict[str, list]:
    """å°†æ¶ˆæ¯æŒ‰ room_id åˆ†ç»„ï¼Œè·³è¿‡ roomid ä¸ºç©º/å¼‚å¸¸çš„è®°å½•"""
    groups: dict[str, list] = defaultdict(list)
    skipped = 0
    for r in records:
        # è·³è¿‡ roomid ä¸ºç©º/None çš„è®°å½•ï¼Œé¿å… room='None' è¿›å…¥åˆ†æé“¾è·¯
        if not r.roomid or r.roomid is None:
            skipped += 1
            continue
        room_id = str(r.roomid)
        # é¢å¤–æ£€æŸ¥ï¼šè·³è¿‡å­—ç¬¦ä¸² 'None' æˆ–ç©ºå­—ç¬¦ä¸²
        if room_id in ('None', ''):
            skipped += 1
            continue
        groups[room_id].append(r)
    if skipped > 0:
        logger.debug(f"[åˆ†ç»„] è·³è¿‡ {skipped} æ¡ roomid ä¸ºç©ºçš„æ¶ˆæ¯")
    return groups


def _merge_chat_context(messages: list) -> str:
    """å°†å¤šæ¡æ¶ˆæ¯åˆå¹¶æˆä¸€ä¸ª chat_contextï¼Œç”¨äºæ‰¹é‡åˆ†æï¼ˆä¸è¿‡æ»¤å™ªéŸ³ï¼Œä¿ç•™å®Œæ•´å¯¹è¯æµï¼‰"""
    lines = []
    for m in messages:
        content = _extract_content(m.msgData)
        if content:
            clean = DataCleanService.sanitize(content)
            if clean:
                # ä¸º LLM è¿›ä¸€æ­¥æ¸…æ´—ï¼šç§»é™¤ @ç”¨æˆ·åã€å¼•ç”¨æ ¼å¼ã€è®¾å¤‡ID ç­‰
                llm_clean = DataCleanService.clean_for_llm(clean)
                if llm_clean:
                    lines.append(llm_clean)
    return "\n".join(lines)


def _get_room_history_context(
    db: Session, 
    room_id: str, 
    limit: int = None,
    min_msgtime: int = None,
) -> tuple[str, list[dict]]:
    """
    ä»æ•°æ®åº“è·å–æˆ¿é—´å†å²æ¶ˆæ¯ä½œä¸ºä¸Šä¸‹æ–‡ã€‚
    ç”¨äºæ‰©å¤§LLMåˆ†æçš„ä¸Šä¸‹æ–‡èŒƒå›´ï¼Œé¿å…å› å½“å‰æ‰¹æ¬¡æ¶ˆæ¯è¿‡å°‘å¯¼è‡´æ€»ç»“ç®€çŸ­ã€‚
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        room_id: æˆ¿é—´ID
        limit: æŸ¥è¯¢çš„æœ€å¤§æ¶ˆæ¯æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½® CONTEXT_HISTORY_COUNT
        min_msgtime: æœ€å°æ¶ˆæ¯æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰ï¼Œç”¨äºè¿‡æ»¤å½“å‰å‘¨æœŸå†…çš„æ¶ˆæ¯
    
    Returns:
        (åˆå¹¶åçš„å†å²æ¶ˆæ¯æ–‡æœ¬, æ¶ˆæ¯åˆ—è¡¨[{msg_id, content}])
        æ¶ˆæ¯åˆ—è¡¨ç”¨äºåç»­æ ¹æ® problem_quote åŒ¹é…å®šä½é—®é¢˜æ¶ˆæ¯
    """
    if limit is None:
        limit = settings.CONTEXT_HISTORY_COUNT
    
    # æ„å»ºæŸ¥è¯¢æ¡ä»¶ï¼ˆä¸å†è¿‡æ»¤ is_noiseï¼Œè®© LLM çœ‹åˆ°å®Œæ•´å¯¹è¯æµï¼ŒåŒ…æ‹¬ç¡®è®¤æ¶ˆæ¯ï¼‰
    filters = [
        WeComMessage.room_id == room_id,
        WeComMessage.msg_type == "text",
    ]
    
    # å¦‚æœæŒ‡å®šäº†æœ€å°æ—¶é—´æˆ³ï¼Œåªè·å–è¯¥æ—¶é—´ä¹‹åçš„æ¶ˆæ¯ï¼ˆå½“å‰å‘¨æœŸå†…ï¼‰
    if min_msgtime:
        # min_msgtime æ˜¯æ¯«ç§’æ—¶é—´æˆ³ï¼Œéœ€è¦è½¬æ¢ä¸º datetime å¯¹è±¡
        min_datetime = datetime.fromtimestamp(min_msgtime / 1000)
        filters.append(WeComMessage.msg_time >= min_datetime)
    
    # æŸ¥è¯¢è¯¥æˆ¿é—´æœ€è¿‘çš„å†å²æ¶ˆæ¯ï¼ˆéå™ªéŸ³ï¼‰
    recent_messages = (
        db.query(WeComMessage)
        .filter(*filters)
        .order_by(WeComMessage.seq.desc())
        .limit(limit)
        .all()
    )
    
    if not recent_messages:
        return "", []
    
    # åè½¬ä¸ºæ—¶é—´é¡ºåºï¼ˆä»æ—§åˆ°æ–°ï¼‰
    recent_messages.reverse()
    
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨å’Œåˆå¹¶å†…å®¹
    msg_list = []
    lines = []
    for msg in recent_messages:
        content = msg.content_clean or msg.content_raw
        if content:
            llm_clean = DataCleanService.clean_for_llm(content)
            if llm_clean:
                lines.append(llm_clean)
                msg_list.append({
                    "msg_id": msg.msg_id,
                    "content": llm_clean,
                })
    
    return "\n".join(lines), msg_list


def _find_best_anchor_msg(msg_list: list[dict], problem_quote: str) -> str | None:
    """
    æ ¹æ® LLM è¿”å›çš„ problem_quoteï¼ˆé—®é¢˜åŸæ–‡å…³é”®å¥ï¼‰ï¼Œåœ¨æ¶ˆæ¯åˆ—è¡¨ä¸­æ¨¡ç³ŠåŒ¹é…æœ€ç›¸å…³çš„æ¶ˆæ¯ã€‚
    
    Args:
        msg_list: æ¶ˆæ¯åˆ—è¡¨ [{msg_id, content}, ...]
        problem_quote: LLM æå–çš„é—®é¢˜åŸæ–‡å…³é”®å¥
    
    Returns:
        æœ€åŒ¹é…æ¶ˆæ¯çš„ msg_idï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å› None
    """
    if not msg_list or not problem_quote:
        return msg_list[0]["msg_id"] if msg_list else None
    
    # æ¸…ç† problem_quote ç”¨äºåŒ¹é…
    quote_clean = problem_quote.strip().lower()
    if len(quote_clean) < 5:
        return msg_list[0]["msg_id"] if msg_list else None
    
    best_match = None
    best_score = 0
    
    for msg in msg_list:
        content = msg.get("content", "").lower()
        if not content:
            continue
        
        # è®¡ç®—åŒ¹é…åˆ†æ•°
        score = 0
        
        # 1. å®Œå…¨åŒ…å«å…³ç³»ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if quote_clean in content:
            score = 100 + len(quote_clean)  # åŒ…å«çš„è¶Šé•¿åˆ†æ•°è¶Šé«˜
        elif content in quote_clean:
            score = 80 + len(content)
        else:
            # 2. å…³é”®è¯åŒ¹é…
            quote_words = set(quote_clean.split())
            content_words = set(content.split())
            common_words = quote_words & content_words
            # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¯
            meaningful_common = [w for w in common_words if len(w) >= 2]
            if meaningful_common:
                score = len(meaningful_common) * 10
        
        if score > best_score:
            best_score = score
            best_match = msg["msg_id"]
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…ï¼Œè¿”å›ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼ˆæœ€æ—©çš„ï¼‰
    if not best_match and msg_list:
        best_match = msg_list[0]["msg_id"]
    
    return best_match


def _find_msg_time_by_quote(
    db: Session, 
    room_id: str, 
    msg_list: list[dict], 
    quote: str
) -> int | None:
    """
    æ ¹æ® LLM è¿”å›çš„å¼•ç”¨å¥ï¼Œåœ¨æ¶ˆæ¯åˆ—è¡¨ä¸­æ¨¡ç³ŠåŒ¹é…æœ€ç›¸å…³çš„æ¶ˆæ¯ï¼Œå¹¶è¿”å›å…¶æ—¶é—´æˆ³ã€‚
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        room_id: æˆ¿é—´ID
        msg_list: æ¶ˆæ¯åˆ—è¡¨ [{msg_id, content}, ...]
        quote: LLM æå–çš„åŸæ–‡å…³é”®å¥
    
    Returns:
        åŒ¹é…æ¶ˆæ¯çš„ msgtimeï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å› None
    """
    if not msg_list or not quote:
        return None
    
    # æ¸…ç† quote ç”¨äºåŒ¹é…
    quote_clean = quote.strip().lower()
    if len(quote_clean) < 3:
        return None
    
    best_match_id = None
    best_score = 0
    
    for msg in msg_list:
        content = msg.get("content", "").lower()
        if not content:
            continue
        
        # è®¡ç®—åŒ¹é…åˆ†æ•°
        score = 0
        
        # 1. å®Œå…¨åŒ…å«å…³ç³»ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if quote_clean in content:
            score = 100 + len(quote_clean)
        elif content in quote_clean:
            score = 80 + len(content)
        else:
            # 2. å…³é”®è¯åŒ¹é…
            quote_words = set(quote_clean.split())
            content_words = set(content.split())
            common_words = quote_words & content_words
            meaningful_common = [w for w in common_words if len(w) >= 2]
            if meaningful_common:
                score = len(meaningful_common) * 10
        
        if score > best_score:
            best_score = score
            best_match_id = msg["msg_id"]
    
    if not best_match_id:
        return None
    
    # æŸ¥è¯¢æ¶ˆæ¯æ—¶é—´
    rec = db.query(ChatRecord.msgtime).filter(
        ChatRecord.roomid == room_id,
        ChatRecord.msgid == str(best_match_id),
    ).first()
    
    return int(rec[0]) if rec and rec[0] else None


def _build_detail_url_with_time_window(
    db: Session, 
    room_id: str, 
    anchor_msg_id: str | None = None,
    anchor_msgtime: int | None = None,
    before_minutes: int = 5,
    use_latest_as_until: bool = True,
    since_msgtime: int | None = None,
    until_msgtime: int | None = None,
) -> str:
    """
    æ„å»ºå¸¦æ—¶é—´çª—å£çš„ detail_urlï¼Œç”¨äºå®šä½åˆ°é—®é¢˜å‘ç”Ÿçš„æ¶ˆæ¯ç‰‡æ®µã€‚
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        room_id: æˆ¿é—´ID
        anchor_msg_id: é”šç‚¹æ¶ˆæ¯IDï¼ˆç”¨äºç¡®å®šæ—¶é—´åŸºå‡†ï¼‰
        anchor_msgtime: é”šç‚¹æ¶ˆæ¯æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        before_minutes: é—®é¢˜å‘ç”Ÿå‰çš„æ—¶é—´çª—å£ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤5åˆ†é’Ÿ
        use_latest_as_until: æ˜¯å¦ä½¿ç”¨è¯¥ç¾¤æœ€æ–°æ¶ˆæ¯ä½œä¸ºç»“æŸæ—¶é—´ï¼ˆç¡®ä¿æ•è·å®Œæ•´å¯¹è¯ï¼‰
        since_msgtime: AI åˆ†æç¡®å®šçš„æ—¶é—´èŒƒå›´èµ·ç‚¹ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰
        until_msgtime: AI åˆ†æç¡®å®šçš„æ—¶é—´èŒƒå›´ç»ˆç‚¹ï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰
    
    Returns:
        å¸¦ since/until å‚æ•°çš„ URLï¼Œå¦‚ï¼š
        {base_url}/api/ui/rooms/{room_id}?since={since_ts}&until={until_ts}
    """
    base_url = f"{settings.INTERNAL_BASE_URL}/api/ui/rooms/{room_id}"
    
    from sqlalchemy import func

    # é€‰å–é”šç‚¹æ—¶é—´ï¼š
    # - ä¼˜å…ˆä½¿ç”¨è°ƒç”¨æ–¹æä¾›çš„ anchor_msgtime
    # - å…¶æ¬¡ç”¨ anchor_msg_id åœ¨ chat_records ä¸­åæŸ¥å¯¹åº” msgtime
    # - æœ€åæ‰å›é€€åˆ°â€œè¯¥ç¾¤æœ€æ–°æ¶ˆæ¯â€ï¼ˆå¯èƒ½ä¸é—®é¢˜æ— å…³ï¼Œä»…å…œåº•ï¼‰
    anchor_ts: int | None = int(anchor_msgtime) if anchor_msgtime else None

    if anchor_ts is None and anchor_msg_id:
        rec = (
            db.query(ChatRecord.msgtime)
            .filter(
                ChatRecord.roomid == room_id,
                ChatRecord.msgid == str(anchor_msg_id),
            )
            .first()
        )
        if rec and rec[0]:
            anchor_ts = int(rec[0])

    # è·å–è¯¥ç¾¤æœ€æ–°æ¶ˆæ¯æ—¶é—´ï¼ˆç”¨äºå…œåº•å’Œè®¡ç®— untilï¼‰
    latest_msg = (
        db.query(func.max(ChatRecord.msgtime).label("max_msgtime"))
        .filter(
            ChatRecord.roomid == room_id,
            ChatRecord.msgtype == "text",
        )
        .first()
    )
    latest_msgtime = int(latest_msg.max_msgtime) if latest_msg and latest_msg.max_msgtime else None

    if anchor_ts is None:
        if not latest_msgtime:
            return base_url
        anchor_ts = latest_msgtime
    
    # è®¡ç®—æ—¶é—´çª—å£ï¼ˆæ¯«ç§’ï¼‰
    # ä¼˜å…ˆä½¿ç”¨ AI åˆ†æç¡®å®šçš„ç²¾ç¡®æ—¶é—´èŒƒå›´
    if since_msgtime:
        # AI ç¡®å®šçš„èµ·ç‚¹ï¼Œå†å¾€å‰ç•™ 2 åˆ†é’Ÿç¼“å†²
        since_ts = since_msgtime - (2 * 60 * 1000)
    else:
        # å…œåº•ï¼šé—®é¢˜å‘ç”Ÿå‰ before_minutes åˆ†é’Ÿ
        since_ts = anchor_ts - (before_minutes * 60 * 1000)
    
    if until_msgtime:
        # AI ç¡®å®šçš„ç»ˆç‚¹ï¼Œå†å¾€åç•™ 2 åˆ†é’Ÿç¼“å†²
        until_ts = until_msgtime + (2 * 60 * 1000)
    elif use_latest_as_until and latest_msgtime:
        # ä½¿ç”¨è¯¥ç¾¤æœ€æ–°æ¶ˆæ¯æ—¶é—´ï¼Œç¡®ä¿æ•è·ä»é—®é¢˜å‘ç”Ÿåˆ°è®¨è®ºç»“æŸçš„å®Œæ•´å¯¹è¯
        until_ts = latest_msgtime
    else:
        # å…œåº•ï¼šé”šç‚¹å 60 åˆ†é’Ÿ
        until_ts = anchor_ts + (60 * 60 * 1000)
    
    return f"{base_url}?since={since_ts}&until={until_ts}"


def _get_last_msgtime(db: Session) -> int:
    state = db.query(IngestState).filter(IngestState.source == SOURCE_KEY).first()
    if not state:
        state = IngestState(source=SOURCE_KEY, last_msgtime=0)
        db.add(state)
        db.commit()
    return int(state.last_msgtime or 0)


def _get_last_seq(db: Session) -> int:
    state = db.query(IngestState).filter(IngestState.source == ARCHIVE_SOURCE_KEY).first()
    if not state:
        state = IngestState(source=ARCHIVE_SOURCE_KEY, last_msgtime=0)
        db.add(state)
        db.commit()
    return int(state.last_msgtime or 0)


def _set_last_msgtime(db: Session, last_msgtime: int) -> None:
    state = db.query(IngestState).filter(IngestState.source == SOURCE_KEY).first()
    if not state:
        state = IngestState(source=SOURCE_KEY, last_msgtime=last_msgtime)
        db.add(state)
    else:
        state.last_msgtime = last_msgtime
    db.commit()


def _set_last_seq(db: Session, last_seq: int) -> None:
    state = db.query(IngestState).filter(IngestState.source == ARCHIVE_SOURCE_KEY).first()
    if not state:
        state = IngestState(source=ARCHIVE_SOURCE_KEY, last_msgtime=last_seq)
        db.add(state)
    else:
        state.last_msgtime = last_seq
    db.commit()


def _fetch_new_messages(db: Session, last_msgtime: int) -> List[ChatRecord]:
    """æ—§ç‰ˆï¼šå…¨å±€æŒ‰æ—¶é—´æ‹‰å–ï¼ˆä¿ç•™å…¼å®¹ï¼‰"""
    return (
        db.query(ChatRecord)
        .filter(ChatRecord.msgtype == "text", ChatRecord.msgtime > last_msgtime)
        .order_by(ChatRecord.msgtime.asc())
        .limit(200)
        .all()
    )


def _fetch_room_messages(
    db: Session, 
    room_id: str, 
    last_msgtime: int, 
    cycle_start_ms: int = 0,
    limit: int = None,
) -> List[ChatRecord]:
    """
    æŒ‰ç¾¤èŠç»´åº¦æ‹‰å–æ¶ˆæ¯ï¼ˆä»…æ‹‰å–å½“å‰å‘¨æœŸå†…çš„æ¶ˆæ¯ï¼‰
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        room_id: ç¾¤èŠID
        last_msgtime: è¯¥ç¾¤å·²å¤„ç†åˆ°çš„æ¸¸æ ‡
        cycle_start_ms: å½“å‰å‘¨æœŸèµ·å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œç”¨äºè¿‡æ»¤å†å²æ¶ˆæ¯
        limit: æœ€å¤§æ‹‰å–æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½® ROOM_FETCH_LIMIT
    
    Returns:
        è¯¥ç¾¤èŠçš„æ–°æ¶ˆæ¯åˆ—è¡¨ï¼ˆä»…åŒ…å« >= max(last_msgtime, cycle_start_ms) çš„æ¶ˆæ¯ï¼‰
    """
    if limit is None:
        limit = settings.ROOM_FETCH_LIMIT
    
    # ä½¿ç”¨ last_msgtime å’Œ cycle_start_ms ä¸­è¾ƒå¤§çš„å€¼ä½œä¸ºè¿‡æ»¤èµ·ç‚¹
    # è¿™ç¡®ä¿ä¸ä¼šå¤„ç†å½“å‰å‘¨æœŸä¹‹å‰çš„å†å²æ¶ˆæ¯
    min_msgtime = max(last_msgtime, cycle_start_ms)
    
    return (
        db.query(ChatRecord)
        .filter(
            ChatRecord.roomid == room_id,
            ChatRecord.msgtype == "text",
            ChatRecord.msgtime > min_msgtime,
        )
        .order_by(ChatRecord.msgtime.asc())
        .limit(limit)
        .all()
    )


def _get_excluded_room_ids(db: Session) -> set:
    """
    è·å–éœ€è¦æ’é™¤çš„ room_id é›†åˆï¼ˆå¸¦ TTL ç¼“å­˜ï¼‰
    è§„åˆ™ï¼šå¦‚æœæŸä¸ªç¾¤å†…æœ‰è¢«æ’é™¤çš„ sender å‘è¿‡æ¶ˆæ¯ï¼Œåˆ™æ•´ä¸ªç¾¤è¢«æ’é™¤
    """
    global _excluded_rooms_cache, _excluded_rooms_cache_ts
    
    exclude_senders = [s.strip() for s in settings.EXCLUDE_SENDERS.split(",") if s.strip()]
    if not exclude_senders:
        return set()
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    ttl = settings.EXCLUDE_ROOMS_CACHE_TTL_SECONDS
    now = time.time()
    if _excluded_rooms_cache and (now - _excluded_rooms_cache_ts) < ttl:
        return _excluded_rooms_cache
    
    # ç¼“å­˜è¿‡æœŸæˆ–ä¸ºç©ºï¼Œé‡æ–°æŸ¥è¯¢
    excluded_rooms = (
        db.query(ChatRecord.roomid)
        .filter(ChatRecord.sender.in_(exclude_senders))
        .distinct()
        .all()
    )
    _excluded_rooms_cache = {r[0] for r in excluded_rooms if r[0]}
    _excluded_rooms_cache_ts = now
    logger.debug(f"[ç¼“å­˜åˆ·æ–°] æ’é™¤ç¾¤åˆ—è¡¨å·²æ›´æ–°ï¼Œå…± {len(_excluded_rooms_cache)} ä¸ªç¾¤ç»„ï¼ŒTTL={ttl}ç§’")
    return _excluded_rooms_cache


def _get_active_mohe_rooms(db: Session, cycle_start_ms: int = 0) -> list[str]:
    """
    è·å–å½“å‰å‘¨æœŸå†…æ´»è·ƒçš„é­”ç›’ç¾¤åˆ—è¡¨ï¼ˆæ’é™¤éé­”ç›’ç¾¤ï¼‰
    
    Args:
        db: æ•°æ®åº“ä¼šè¯
        cycle_start_ms: å½“å‰å‘¨æœŸèµ·å§‹æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œåªè€ƒè™‘ msgtime > cycle_start_ms çš„æ¶ˆæ¯æ‰€åœ¨çš„ç¾¤
    
    Returns:
        å½“å‰å‘¨æœŸå†…æœ‰æ–°æ¶ˆæ¯çš„é­”ç›’ç¾¤ room_id åˆ—è¡¨
    """
    # æŸ¥è¯¢å½“å‰å‘¨æœŸå†…æœ‰æ–°æ¶ˆæ¯çš„æ‰€æœ‰ç¾¤èŠ
    query = db.query(ChatRecord.roomid).filter(
        ChatRecord.msgtype == "text",
        ChatRecord.msgtime > cycle_start_ms,
    ).distinct()
    
    all_rooms = {r[0] for r in query.all() if r[0] and r[0] not in ('None', '')}
    
    # æ’é™¤éé­”ç›’ç¾¤
    excluded_rooms = _get_excluded_room_ids(db)
    mohe_rooms = all_rooms - excluded_rooms
    
    logger.debug(
        f"[æ´»è·ƒç¾¤èŠ] å‘¨æœŸèµ·ç‚¹={datetime.fromtimestamp(cycle_start_ms/1000) if cycle_start_ms else 'N/A'}, "
        f"æ€»è®¡ {len(all_rooms)} ä¸ªç¾¤ï¼Œæ’é™¤ {len(excluded_rooms)} ä¸ªéé­”ç›’ç¾¤ï¼Œå‰©ä½™ {len(mohe_rooms)} ä¸ªé­”ç›’ç¾¤"
    )
    return list(mohe_rooms)


def _archive_new_messages(db: Session, wecom: WeComService) -> None:
    if not settings.WECOM_ARCHIVE_ENABLED:
        return

    last_seq = _get_last_seq(db)
    records = wecom.fetch_messages(after_seq=last_seq, limit=settings.WECOM_ARCHIVE_LIMIT)
    if not records:
        return

    max_seq = last_seq
    for item in records:
        msg_id = str(item.get("msgid") or "")
        if not msg_id:
            continue
        exists = db.query(ChatRecord).filter(ChatRecord.msgid == msg_id).first()
        if exists:
            continue

        seq = int(item.get("seq") or 0)
        msgtime = int(item.get("msgtime") or 0)
        if seq:
            max_seq = max(max_seq, seq)

        record = ChatRecord(
            msgid=msg_id,
            action=item.get("action") or "",
            sender=item.get("from") or "",
            tolist=item.get("tolist") or "",
            roomid=item.get("roomid") or "",
            msgtime=msgtime,
            msgtype=item.get("msgtype") or "",
            msgData=item.get("msgData") or "",
            seq=seq,
        )
        db.add(record)

    db.commit()
    _set_last_seq(db, max_seq)


def _resolve_assignee(db: Session, room_id: str, issue_type: str | None) -> str:
    type_mapping = {
        "ä½¿ç”¨å’¨è¯¢": settings.ISSUE_TYPE_ASSIGNEE_USAGE,
        "é—®é¢˜åé¦ˆ": settings.ISSUE_TYPE_ASSIGNEE_FEEDBACK,
        "äº§å“éœ€æ±‚": settings.ISSUE_TYPE_ASSIGNEE_REQUIREMENT,
        "äº§å“ç¼ºé™·": settings.ISSUE_TYPE_ASSIGNEE_DEFECT,
    }
    mapped = type_mapping.get(issue_type or "")
    if mapped:
        return mapped
    mapping = db.query(RoomAssignee).filter(RoomAssignee.room_id == room_id).first()
    if mapping and mapping.assignee:
        return mapping.assignee
    return settings.DEFAULT_ASSIGNEE


def _resolve_room_name(db: Session, room_id: str) -> str:
    if not room_id:
        return room_id
    mapping = db.query(RoomInfo).filter(RoomInfo.room_id == room_id).first()
    if mapping and mapping.room_name:
        return mapping.room_name
    return room_id


async def process_message(
    db: Session,
    *,
    msg_id: str,
    room_id: str,
    sender_id: str,
    msg_type: str,
    clean_text: str,
    raw_text: str,
    sentinel: SentinelAgent,
    assistant: AssistantAgent,
    wecom: WeComService,
    allow_reply: bool = True,
    allow_alert: bool = True,
    allow_ticket: bool = True,
    replay: bool = False,
    room_name: str | None = None,
    since_msgtime: int | None = None,
    until_msgtime: int | None = None,
) -> dict:
    if not clean_text:
        return {"status": "ignored", "reason": "empty"}
    if DataCleanService.is_noise(clean_text):
        return {"status": "ignored", "reason": "noise"}

    # è·å–æœ€è¿‘çš„å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œç”¨äºåˆ¤æ–­é—®é¢˜æ˜¯å¦å·²è§£å†³
    recent_chat_lines = data_service.get_recent_chat_text(db, room_id, limit=10)
    if not recent_chat_lines:
        recent_chat_lines = data_service.get_recent_wecom_text(db, room_id, limit=10)

    analysis = await sentinel.check_message(clean_text)
    
    # è°ƒç”¨å®Œæ•´åˆ†æè·å–50å­—è¯¦ç»†æ€»ç»“ï¼ˆç”¨äºé’‰é’‰æ¨é€ï¼‰
    complete_analysis = await analyze_complete_llm(clean_text)
    summary_50 = complete_analysis.get("summary") or ""  # 50å­—è¯¦ç»†æ€»ç»“
    
    is_hard = is_hard_issue(clean_text, analysis, chat_lines=recent_chat_lines)
    is_resolved = check_resolved_status(clean_text, chat_lines=recent_chat_lines)
    resolved_suffix = "ï¼ˆå·²è§£å†³ï¼‰" if is_resolved else ""
    reply_text = await generate_reply(clean_text)
    reply_mode = "auto" if (
        settings.AUTO_REPLY_ENABLED and analysis.get("risk_score", 0) <= settings.AUTO_REPLY_MAX_RISK
    ) else "suggest"
    # ä¼˜å…ˆä½¿ç”¨å®Œæ•´åˆ†æçš„ issue_type å’Œ priorityï¼ˆæ›´å‡†ç¡®ï¼‰
    issue_type = normalize_issue_type(complete_analysis.get("issue_type") or analysis.get("issue_type"))
    priority_from_llm = normalize_priority(complete_analysis.get("priority") or "")
    assignee = _resolve_assignee(db, room_id, issue_type)
    room_name = room_name or _resolve_room_name(db, room_id)

    summary_text = clean_text[: settings.ISSUE_SUMMARY_LEN]
    evidence_id = f"replay:{msg_id}" if replay else str(msg_id)
    issue = Issue(
        room_id=room_id,
        summary=summary_text,
        issue_type=issue_type,
        category=f"{analysis.get('category_l1', 'OTHER')}/{analysis.get('category_l2', 'OTHER')}",
        category_l1=analysis.get("category_l1"),
        category_l2=analysis.get("category_l2"),
        category_short=analysis.get("category_short"),
        labels=analysis.get("labels") or [],
        severity=analysis.get("severity"),
        confidence=int((analysis.get("confidence") or 0) * 100),
        taxonomy_version=analysis.get("taxonomy_version"),
        classification_strategy=analysis.get("classification_strategy"),
        risk_score=analysis.get("risk_score", 0),
        is_bug=bool(analysis.get("is_bug")),
        suggested_reply=reply_text,
        reply_mode=reply_mode,
        evidence=[evidence_id],
        status="alerted" if (is_hard and analysis.get("is_alert")) else "pending",
    )
    db.add(issue)
    db.commit()
    update_issue_aggregation(
        db,
        issue,
        is_hard=is_hard,
        is_alert=bool(analysis.get("is_alert")),
    )

    draft = None
    # æ„å»ºå¸¦æ—¶é—´çª—å£çš„é“¾æ¥ï¼Œå®šä½åˆ°é—®é¢˜æ¶ˆæ¯ç‰‡æ®µ
    # ä¼˜å…ˆä½¿ç”¨ AI åˆ†æç¡®å®šçš„æ—¶é—´èŒƒå›´ï¼Œå…œåº•ä½¿ç”¨ msg_id ä¸ºé”šç‚¹
    detail_url = _build_detail_url_with_time_window(
        db, room_id, 
        anchor_msg_id=msg_id,
        since_msgtime=since_msgtime,
        until_msgtime=until_msgtime,
    )
    
    # è·å–é—®é¢˜å‘ç”Ÿæ—¶é—´ï¼ˆä» msg_id åæŸ¥ ChatRecord.msgtimeï¼‰
    issue_time = None
    issue_msgtime = None
    anchor_rec = db.query(ChatRecord.msgtime).filter(
        ChatRecord.roomid == room_id,
        ChatRecord.msgid == str(msg_id),
    ).first()
    if anchor_rec and anchor_rec[0]:
        issue_msgtime = int(anchor_rec[0])
        issue_time = _format_issue_time(issue_msgtime)
    
    # æ—¶é—´æ ¡éªŒï¼šè·³è¿‡å½“å‰å‘¨æœŸä¹‹å‰çš„å†å²é—®é¢˜æ¨é€
    cycle_start_ms = _get_current_cycle_start()
    if issue_msgtime and issue_msgtime < cycle_start_ms:
        logger.info(
            f"[æ—¶é—´è¿‡æ»¤] è·³è¿‡å†å²é—®é¢˜æ¨é€: room={room_id}, "
            f"issue_time={issue_time}, cycle_start={datetime.fromtimestamp(cycle_start_ms/1000)}"
        )
        # è·³è¿‡æ¨é€ä½†ä»è®°å½• Issueï¼ˆallow_alert è®¾ä¸º Falseï¼‰
        allow_alert = False
    
    # è‰ç¨¿é˜¶æ®µå°±å‡†å¤‡å¥½"ç°è±¡/å…³é”®å¥"ï¼Œé¿å…åç»­å»ºå•åªæœ‰ä¸€å¥è¯
    # ä¼˜å…ˆä½¿ç”¨å®Œæ•´åˆ†æç»“æœï¼ˆæ›´å‡†ç¡®ï¼‰ï¼Œå…œåº•ä½¿ç”¨ Sentinel ç»“æœ
    phenomenon_text = complete_analysis.get("phenomenon") or analysis.get("phenomenon") or (clean_text[:50] if clean_text else "")
    key_sentence_text = analysis.get("key_sentence") or (clean_text.split("\n")[0][:100] if clean_text else "")
    # summary_50 ç”¨äºé’‰é’‰æ¨é€ï¼ˆ50å­—è¯¦ç»†æ€»ç»“ï¼‰ï¼Œkey_sentence_text ç”¨äºTBå¤‡æ³¨ï¼ˆç®€çŸ­ï¼‰
    summary_for_alert = summary_50 if summary_50 else key_sentence_text
    if allow_ticket and is_hard:  # is_hard å·²åŒ…å« severity/is_bug/å…³é”®è¯/RAG åˆ¤æ–­
        draft_content = build_ticket_draft(
            room_id=room_id,
            summary=summary_text,
            category=f"{analysis.get('category_l1', 'OTHER')}/{analysis.get('category_l2', 'OTHER')}",
            severity=analysis.get("severity", "S1"),
            risk_score=analysis.get("risk_score", 0),
            raw_text=clean_text,
            room_name=room_name,
            customer=sender_id,
            detail_url=detail_url,
            phenomenon=phenomenon_text,
            key_sentence=key_sentence_text,
            suggested_reply=reply_text,
            platform=complete_analysis.get("platform"),  # ç«¯å£åˆ†ç±»
        )
        # ä½¿ç”¨ LLM ç”Ÿæˆè¯¦ç»†æ ‡é¢˜ï¼ˆ30-40å­—ï¼‰ï¼Œä½¿ç”¨50å­—æ€»ç»“ä½œä¸ºè¾“å…¥
        llm_title = await generate_ticket_title_llm(
            phenomenon=phenomenon_text,
            summary=summary_for_alert,  # ä½¿ç”¨è¯¦ç»†æ€»ç»“ç”Ÿæˆæ›´å¥½çš„æ ‡é¢˜
        )
        if llm_title:
            llm_title = llm_title + resolved_suffix  # å·²è§£å†³é—®é¢˜æ·»åŠ æ ‡è®°
            draft_content["llm_title"] = llm_title
            draft_content["title"] = llm_title
        elif resolved_suffix:
            # å¦‚æœæ²¡æœ‰ LLM æ ‡é¢˜ä½†é—®é¢˜å·²è§£å†³ï¼Œä¹Ÿè¦åœ¨åŸå§‹æ ‡é¢˜ä¸Šæ·»åŠ æ ‡è®°
            draft_content["title"] = draft_content.get("title", "") + resolved_suffix
        draft = TicketDraft(
            issue_id=issue.issue_id,
            room_id=room_id,  # æ·»åŠ  room_id ç”¨äºå»é‡æ£€æŸ¥
            title=(llm_title or draft_content.get("title")),
            severity=draft_content.get("severity"),
            category=draft_content.get("category", ""),
            content=draft_content,
            status="draft",
            assigned_to=assignee,
        )
        db.add(draft)
        db.commit()

    if allow_reply and reply_mode == "auto":
        wecom.send_reply(room_id, reply_text)

    send_alert, alert_level, alert_event = should_send_alert(
        db=db,
        room_id=room_id,
        category_l1=analysis.get("category_l1", "OTHER"),
        category_l2=analysis.get("category_l2", "OTHER"),
        severity=analysis.get("severity"),
        risk_score=analysis.get("risk_score", 0),
        is_alert=bool(analysis.get("is_alert")),
        is_bug=bool(analysis.get("is_bug")),
    )
    
    # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºæ¨é€åˆ¤æ–­æ¡ä»¶
    if not (allow_alert and send_alert and is_hard):
        logger.debug(
            f"[æ¨é€è·³è¿‡] room={room_id}, "
            f"allow_alert={allow_alert}, send_alert={send_alert}, is_hard={is_hard}, "
            f"severity={analysis.get('severity')}, is_bug={analysis.get('is_bug')}, "
            f"risk_score={analysis.get('risk_score', 0)}, is_alert={analysis.get('is_alert')}"
        )
    
    if allow_alert and send_alert and is_hard:
        # å·²ç§»é™¤ã€ğŸ§  AI æ™ºèƒ½è¾…åŠ©ã€‘ç›¸å…³é€»è¾‘ï¼šä¸å†åšç›¸ä¼¼æ¡ˆä¾‹æ£€ç´¢/æ·±åº¦åˆ†æ/æ–¹æ¡ˆä¸å®‰æŠšè¯æœ¯ç”Ÿæˆ
        # ä»…åŸºäºæ¸…æ´—åçš„æ–‡æœ¬ï¼Œæ„å»ºâ€œå®¢æˆ·åŸå£°æ‘˜è¦â€
        # ä½¿ç”¨ Sentinel AI ç”Ÿæˆçš„ç®€çŸ­æ‘˜è¦ï¼ˆ50å­—ä»¥å†…çš„ç°è±¡ + ä¸€å¥å…³é”®å¥ï¼‰
        aggregate_summary = build_aggregate_summary(
            db=db,
            room_id=room_id,
            category_l1=analysis.get("category_l1", "OTHER"),
            category_l2=analysis.get("category_l2", "OTHER"),
            since_time=alert_event.first_seen_at if alert_event else None,
            limit=settings.ALERT_AGGREGATE_LIMIT,
        )
        if draft:
            content = draft.content or {}
            # å°† risk_score è½¬æ¢ä¸º priority
            risk_score = analysis.get("risk_score", 0)
            priority = risk_score_to_priority(risk_score)
            # ä¼˜å…ˆä½¿ç”¨ LLM åˆ†æçš„ priorityï¼Œå…œåº•ä½¿ç”¨ risk_score è½¬æ¢
            priority = priority_from_llm if priority_from_llm else risk_score_to_priority(risk_score)
            content.update(
                {
                    "issue_type": issue_type,
                    "priority": priority,
                    "severity": analysis.get("severity", "-") or "-",
                    "risk_score": risk_score,
                    "category": f"{analysis.get('category_l1', 'OTHER')}/{analysis.get('category_l2', 'OTHER')}",
                    "category_short": analysis.get("category_short"),
                    "phenomenon": phenomenon_text,
                    "summary": summary_for_alert,  # ä½¿ç”¨50å­—è¯¦ç»†æ€»ç»“
                    "key_sentence": key_sentence_text,
                    "detail_url": detail_url,
                    "room_name": room_name,
                    "room_id": room_id,
                    "customer": sender_id,
                    "suggested_reply": reply_text,
                    "issue_time": issue_time,  # é—®é¢˜å‘ç”Ÿæ—¶é—´ï¼ˆæ ¼å¼åŒ–åï¼‰
                }
            )
            # æ„å»ºæ–°æ ¼å¼çš„é’‰é’‰ markdownï¼ˆå·²è§£å†³é—®é¢˜åœ¨ç°è±¡åæ·»åŠ æ ‡è®°ï¼‰
            phenomenon_with_suffix = phenomenon_text + resolved_suffix if resolved_suffix else phenomenon_text
            content["dingtalk_markdown"] = build_ticket_markdown(
                content,
                issue_type=issue_type,
                priority=priority,
                phenomenon=phenomenon_with_suffix,  # å·²è§£å†³é—®é¢˜æ·»åŠ æ ‡è®°
                summary=summary_for_alert,  # ä½¿ç”¨50å­—è¯¦ç»†æ€»ç»“
                room_name=room_name,
                detail_url=detail_url,
                issue_time=issue_time,  # é—®é¢˜å‘ç”Ÿæ—¶é—´
            )
            content.pop("customfields_pending", None)
            # è‰ç¨¿é˜¶æ®µå·²ç”Ÿæˆ llm_titleï¼›æ­¤å¤„å…œåº•
            title_text = content.get("llm_title") or build_ticket_title(content)
            draft.title = title_text
            content["title"] = title_text

        # å‘é€é’‰é’‰æ¨é€ï¼ˆä½¿ç”¨æ–°çš„ä¼˜å…ˆçº§æ ¼å¼ï¼‰
        dingtalk_markdown = content.get("dingtalk_markdown") if (draft and content) else None
        priority = content.get("priority") if (draft and content) else risk_score_to_priority(analysis.get("risk_score", 0))
        DingTalkService.send_alert(
            summary=aggregate_summary or clean_text,
            risk=analysis.get("risk_score", 0),
            reason=analysis.get("reason", ""),
            room_id=room_id,
            room_name=room_name,
            issue_type=issue_type,
            priority=priority,
            phenomenon=phenomenon_text,
            detail_url=detail_url,
            markdown_text=dingtalk_markdown,
        )

        # TB AIè¾…åŠ©å­—æ®µï¼šç­‰äºé’‰é’‰æ¨é€å†…å®¹çš„çº¯æ–‡æœ¬ç‰ˆæœ¬ï¼ˆä¸å«ã€ğŸ§  AI æ™ºèƒ½è¾…åŠ©ã€‘ï¼‰
        if draft and content and isinstance(content.get("dingtalk_markdown"), str):
            content["ai_assistant"] = markdown_to_plain_text(content["dingtalk_markdown"])
            content["ai_assistant_text"] = content["ai_assistant"]
            build_customfields_pending(content)
            draft.content = content
            flag_modified(draft, 'content')  # å…³é”®ï¼šç¡®ä¿ JSON å­—æ®µå®Œæ•´æŒä¹…åŒ–
            db.commit()
            db.refresh(draft)  # å…³é”®ï¼šåˆ·æ–°å¯¹è±¡ï¼Œç¡®ä¿åç»­è®¿é—®ä½¿ç”¨æœ€æ–°æ•°æ®

        if allow_ticket and draft and settings.TEAMBITION_AUTO_CREATE and not draft.teambition_ticket_id:
            # ä½¿ç”¨ LLM ç”Ÿæˆç²¾ç‚¼çš„é—®é¢˜æ¦‚æ‹¬ï¼ˆç”¨äºTBå¤‡æ³¨/æ ‡é¢˜å…œåº•ï¼‰
            if isinstance(draft.content, dict) and not draft.content.get("llm_note_summary"):
                draft.content["llm_note_summary"] = await generate_note_summary_llm(clean_text, max_len=30)
                flag_modified(draft, 'content')  # å…³é”®ï¼šæ ‡è®°JSONå­—æ®µå·²ä¿®æ”¹ï¼Œç¡®ä¿SQLAlchemyæŒä¹…åŒ–åµŒå¥—æ›´æ”¹
                db.commit()
                db.refresh(draft)  # åˆ·æ–°ç¡®ä¿æ•°æ®åŒæ­¥

            # æ–°é¡¹ç›®å­—æ®µï¼šå®¢æˆ·ç«¯ç‰ˆæœ¬ / CBSç‰ˆæœ¬ / é•œåƒIDï¼ˆä»å¯¹è¯ä¸­æŠ½å–ï¼‰
            if isinstance(draft.content, dict) and (
                not draft.content.get("client_version")
                or not draft.content.get("cbs_version")
                or not draft.content.get("image_id")
            ):
                extracted = await extract_versions_and_image_llm(clean_text)
                if isinstance(extracted, dict):
                    draft.content["client_version"] = extracted.get("client_version") or "-"
                    draft.content["cbs_version"] = extracted.get("cbs_version") or "-"
                    draft.content["image_id"] = extracted.get("image_id") or "-"
                flag_modified(draft, 'content')  # å…³é”®ï¼šæ ‡è®°JSONå­—æ®µå·²ä¿®æ”¹
                db.commit()
                db.refresh(draft)  # åˆ·æ–°ç¡®ä¿æ•°æ®åŒæ­¥

            if settings.TEAMBITION_MODE == "api":
                ticket_id = create_task(draft.title or "è‡ªåŠ¨å·¥å•", (draft.content or {}).get("description", ""))
                if ticket_id:
                    draft.teambition_ticket_id = ticket_id
                    draft.status = "ticketed"
                    db.commit()
            elif settings.TEAMBITION_MODE == "oapi":
                payload = build_task_payload(
                    draft.title or "è‡ªåŠ¨å·¥å•",
                    (draft.content or {}).get("description", ""),
                    draft.content,
                )
                if payload:
                    payload["customfields"] = build_customfields_for_create(draft.content or {})
                    ticket_id = create_task_oapi(payload)
                    if ticket_id:
                        draft.teambition_ticket_id = ticket_id
                        draft.status = "ticketed"
                        db.commit()
                        for item in (draft.content or {}).get("customfields_pending") or []:
                            update_task_customfield(ticket_id, item)
            elif settings.TEAMBITION_MODE == "mcp":
                # MCP æ¨¡å¼ï¼šè‡ªåŠ¨æäº¤å»ºå•è¯·æ±‚ï¼ˆä¸ /mcp_request é€»è¾‘ä¸€è‡´ï¼‰
                payload = build_task_payload(
                    draft.title or "è‡ªåŠ¨å·¥å•",
                    (draft.content or {}).get("description", ""),
                    draft.content if isinstance(draft.content, dict) else None,
                )
                if payload:
                    draft.mcp_status = "pending"
                    draft.mcp_payload = payload
                    draft.mcp_requested_at = datetime.utcnow()
                    db.commit()
                    bridge_result = submit_mcp_task(payload)
                    if bridge_result and bridge_result.get("ticket_id"):
                        draft.teambition_ticket_id = bridge_result.get("ticket_id")
                        draft.status = "ticketed"
                        draft.mcp_status = "completed"
                        draft.mcp_completed_at = datetime.utcnow()
                        db.commit()
            else:
                logger.warning("TEAMBITION_MODE æœªè®¾ç½®ä¸º mcp/oapi/apiï¼Œå·²è·³è¿‡å»ºå•")

            # å»ºå•æˆåŠŸåï¼šè¡¥å‘â€œå·¥å•å·²åˆ›å»ºâ€é€šçŸ¥ï¼ˆå¸¦å·¥å•é“¾æ¥ï¼‰
            if draft.teambition_ticket_id:
                ticket_url = get_task_url(draft.teambition_ticket_id)
                if isinstance(draft.content, dict) and ticket_url:
                    content = draft.content
                    # ä½¿ç”¨æ–°æ ¼å¼æ„å»º markdown
                    content["dingtalk_markdown"] = build_ticket_markdown(
                        content,
                        issue_type=content.get("issue_type") or "é—®é¢˜åé¦ˆ",
                        priority=content.get("priority") or risk_score_to_priority(int(content.get("risk_score") or 0)),
                        phenomenon=content.get("phenomenon"),
                        summary=content.get("summary") or content.get("key_sentence"),
                        room_name=content.get("room_name") or room_name,
                        detail_url=content.get("detail_url") or f"{settings.INTERNAL_BASE_URL}/api/ui/rooms/{content.get('room_id')}",
                    )
                    draft.content = content
                    db.commit()
                # æŒ‰éœ€æ±‚ï¼šä¸€ä¸ªé—®é¢˜åªæ¨é€ä¸€æ¡é’‰é’‰æ¶ˆæ¯ï¼ˆä¸å†è¡¥å‘â€œå·¥å•å·²åˆ›å»ºâ€äºŒæ¬¡æ¨é€ï¼‰

    return {"status": "alerted" if (is_hard and send_alert) else "saved", "issue_id": issue.issue_id}


async def polling_loop():
    """
    ç¾¤èŠç»´åº¦è½®è¯¢å¾ªç¯ï¼ˆæ¯æ—¥å‘¨æœŸç‰ˆï¼‰ï¼š
    1. æ¯å¤© DAILY_CYCLE_START_HOUR ç‚¹å¼€å§‹æ–°å‘¨æœŸï¼Œé‡ç½®æ‰€æœ‰ç¾¤èŠçŠ¶æ€
    2. åªå¤„ç†å½“å‰å‘¨æœŸå†…çš„æ¶ˆæ¯ï¼ˆå¿½ç•¥å†å²æ•°æ®ï¼‰
    3. è·å–æ´»è·ƒé­”ç›’ç¾¤ï¼ŒæŒ‰ä¸Šæ¬¡å¤„ç†æ—¶é—´æ’åºï¼ˆå…¬å¹³è½®è¯¢ï¼‰
    4. å¯¹æ¯ä¸ªç¾¤èŠå•ç‹¬æ‹‰å–æ¶ˆæ¯å¹¶å…¥åº“
    5. ç´¯ç§¯æœªåˆ†ææ¶ˆæ¯æ•°ï¼Œè¾¾åˆ°é˜ˆå€¼æ‰è§¦å‘ LLM åˆ†æ
    6. å»ºå•å‰æ£€æŸ¥å»é‡ï¼ˆåŒç¾¤åŒå‘¨æœŸå†…ç›¸ä¼¼é—®é¢˜ä¸é‡å¤å»ºå•ï¼‰
    7. é«˜é£é™©å…³é”®è¯å¯ç»•è¿‡å†·å´æœŸ
    8. åˆ†æåé‡ç½®ç´¯ç§¯è®¡æ•°å¹¶æ›´æ–°å†·å´æ—¶é—´
    9. ç¾¤èŠçŠ¶æ€æŒä¹…åŒ–åˆ°æ•°æ®åº“ï¼ŒæœåŠ¡é‡å¯åä»ä¸Šæ¬¡ä½ç½®ç»§ç»­
    """
    if not settings.POLLING_ENABLED:
        logger.warning("[è½®è¯¢] POLLING_ENABLED=falseï¼Œè½®è¯¢æœªå¯åŠ¨")
        return

    cycle_start_ms = _get_current_cycle_start()
    logger.info(
        f"[ç¾¤èŠè½®è¯¢] å¯åŠ¨è½®è¯¢æœåŠ¡ï¼ˆæ¯æ—¥å‘¨æœŸæ¨¡å¼ï¼‰ï¼Œ"
        f"å‘¨æœŸèµ·ç‚¹={settings.DAILY_CYCLE_START_HOUR}:00, "
        f"å½“å‰å‘¨æœŸ={datetime.fromtimestamp(cycle_start_ms/1000)}, "
        f"é—´éš”={settings.POLLING_INTERVAL_SECONDS}ç§’, "
        f"æœ‰æ•ˆæ¶ˆæ¯é˜ˆå€¼={settings.ROOM_MIN_MESSAGES_FOR_ANALYZE}, "
        f"åŸå§‹æ¶ˆæ¯é˜ˆå€¼={settings.ROOM_RAW_MIN_MESSAGES_FOR_ANALYZE}, "
        f"é«˜é£é™©æœ€å°‘æ¶ˆæ¯={settings.HIGH_RISK_MIN_MESSAGES}, "
        f"å†·å´æ—¶é—´={settings.ROOM_COOLDOWN_SECONDS}ç§’, "
        f"å»é‡é˜ˆå€¼={settings.ISSUE_DEDUP_SIMILARITY_THRESHOLD}"
    )

    sentinel = SentinelAgent()
    assistant = AssistantAgent()
    wecom = WeComService()
    faq_service = FaqService()
    
    # æ ‡è®°æ˜¯å¦å·²ä»æ•°æ®åº“åŠ è½½çŠ¶æ€ï¼ˆä»…é¦–æ¬¡å¯åŠ¨æ—¶åŠ è½½ï¼‰
    state_loaded = False

    while True:
        db = SessionLocal()
        try:
            # ========== é¦–æ¬¡å¯åŠ¨æ—¶ä»æ•°æ®åº“åŠ è½½çŠ¶æ€ ==========
            if not state_loaded:
                loaded_count = _load_all_room_states(db)
                logger.info(f"[ç¾¤èŠè½®è¯¢] ä»æ•°æ®åº“æ¢å¤äº† {loaded_count} ä¸ªç¾¤èŠçš„è½®è¯¢çŠ¶æ€")
                state_loaded = True
            
            _archive_new_messages(db, wecom)
            
            # ========== æ¯æ—¥å‘¨æœŸæ£€æµ‹ä¸é‡ç½® ==========
            _check_and_reset_cycle(db)  # ä¼ å…¥ db ä»¥æ”¯æŒæŒä¹…åŒ–é‡ç½®åçš„çŠ¶æ€
            cycle_start_ms = _get_current_cycle_start()
            
            # ========== ä¼˜å…ˆå¤„ç† needs_flush çš„ç¾¤èŠï¼ˆå‘¨æœŸé‡ç½®å‰æœªåˆ†æçš„ï¼‰ ==========
            flush_rooms = [
                rid for rid, state in _room_state.items()
                if state.get("needs_flush")
            ]
            if flush_rooms:
                logger.info(f"[å‘¨æœŸé‡ç½®å…œåº•] ä¼˜å…ˆå¤„ç† {len(flush_rooms)} ä¸ª needs_flush ç¾¤èŠ")
                for flush_room_id in flush_rooms:
                    flush_state = _get_room_state(flush_room_id)
                    # ä½¿ç”¨ä¸Šä¸€ä¸ªå‘¨æœŸçš„èµ·ç‚¹æ¥è·å–ä¸Šä¸‹æ–‡ï¼ˆå› ä¸ºæ¶ˆæ¯å±äºä¸Šä¸ªå‘¨æœŸï¼‰
                    prev_cycle_ms = cycle_start_ms - 86400000  # å‰ä¸€å¤©åŒä¸€æ—¶é—´
                    expanded_context, msg_list = _get_room_history_context(db, flush_room_id, min_msgtime=prev_cycle_ms)
                    if expanded_context:
                        logger.info(
                            f"[å‘¨æœŸé‡ç½®å…œåº•] room={flush_room_id}, "
                            f"pending={flush_state.get('pending_count', 0)}, "
                            f"raw={flush_state.get('raw_pending_count', 0)}, "
                            f"ä¸Šä¸‹æ–‡={len(expanded_context)}å­—ç¬¦"
                        )
                        pre_analysis = await analyze_complete_llm(expanded_context)
                        phenomenon = pre_analysis.get("phenomenon", "")
                        problem_quote = pre_analysis.get("problem_quote", "")
                        if phenomenon and phenomenon != "æš‚æ— ":
                            if not await _is_duplicate_issue(db, flush_room_id, phenomenon, prev_cycle_ms):
                                anchor_msg_id = _find_best_anchor_msg(msg_list, problem_quote)
                                if not anchor_msg_id and msg_list:
                                    anchor_msg_id = msg_list[0]["msg_id"]
                                since_msgtime = _find_msg_time_by_quote(db, flush_room_id, msg_list, pre_analysis.get("first_problem_quote", ""))
                                until_msgtime = _find_msg_time_by_quote(db, flush_room_id, msg_list, pre_analysis.get("last_discussion_quote", ""))
                                await process_message(
                                    db,
                                    msg_id=anchor_msg_id or f"flush_{flush_room_id}_{int(time.time()*1000)}",
                                    room_id=flush_room_id,
                                    sender_id="system",
                                    msg_type="text",
                                    clean_text=expanded_context,
                                    raw_text=expanded_context,
                                    sentinel=sentinel,
                                    assistant=assistant,
                                    wecom=wecom,
                                    allow_reply=False,
                                    allow_alert=True,
                                    allow_ticket=True,
                                    replay=False,
                                    since_msgtime=since_msgtime,
                                    until_msgtime=until_msgtime,
                                )
                    # æ¸…é™¤ needs_flush æ ‡è®°å¹¶é‡ç½®è®¡æ•°
                    flush_state["needs_flush"] = False
                    flush_state["pending_count"] = 0
                    flush_state["raw_pending_count"] = 0
                    _save_room_state(db, flush_room_id)
            
            # ========== ç¾¤èŠç»´åº¦è½®è¯¢ ==========
            # 1. è·å–å½“å‰å‘¨æœŸå†…æ´»è·ƒçš„é­”ç›’ç¾¤
            active_rooms = _get_active_mohe_rooms(db, cycle_start_ms=cycle_start_ms)
            
            # 2. æŒ‰ä¸Šæ¬¡å¤„ç†æ—¶é—´æ’åºï¼ˆæœ€ä¹…æœªå¤„ç†çš„ä¼˜å…ˆï¼‰
            active_rooms.sort(key=lambda r: _get_room_state(r).get("last_processed_at", 0))
            
            # 3. é™åˆ¶æ¯è½®å¤„ç†çš„ç¾¤èŠæ•°
            rooms_to_process = active_rooms[:settings.MAX_ROOMS_PER_ROUND]
            
            if not rooms_to_process:
                logger.debug("[ç¾¤èŠè½®è¯¢] æœ¬è½®æ— æ´»è·ƒé­”ç›’ç¾¤")
            else:
                logger.info(f"[ç¾¤èŠè½®è¯¢] æœ¬è½®å¤„ç† {len(rooms_to_process)}/{len(active_rooms)} ä¸ªé­”ç›’ç¾¤")
            
            vector_messages = []
            analyzed_count = 0
            
            # 4. éå†æ¯ä¸ªç¾¤èŠ
            for room_id in rooms_to_process:
                state = _get_room_state(room_id)
                
                # 5. æ‹‰å–è¯¥ç¾¤æ–°æ¶ˆæ¯ï¼ˆä»…å½“å‰å‘¨æœŸå†…ï¼‰
                records = _fetch_room_messages(db, room_id, state["last_msgtime"], cycle_start_ms)
                
                # ========== æƒ…å†µAï¼šæ²¡æœ‰æ–°æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦ç´¯ç§¯è§¦å‘ ==========
                if not records:
                    pending_count = state["pending_count"]
                    raw_pending = state.get("raw_pending_count", 0)
                    in_cooldown = _is_in_cooldown(room_id)
                    
                    # åŒé˜ˆå€¼è§¦å‘ï¼šæœ‰æ•ˆæ¶ˆæ¯æ•°>=é˜ˆå€¼ OR åŸå§‹æ¶ˆæ¯æ•°>=åŸå§‹é˜ˆå€¼
                    should_trigger = (
                        not in_cooldown and (
                            pending_count >= settings.ROOM_MIN_MESSAGES_FOR_ANALYZE
                            or raw_pending >= settings.ROOM_RAW_MIN_MESSAGES_FOR_ANALYZE
                        )
                    )
                    if should_trigger:
                        trigger_detail = (
                            f"æœ‰æ•ˆ={pending_count}, åŸå§‹={raw_pending}, "
                            f"é˜ˆå€¼: æœ‰æ•ˆ>={settings.ROOM_MIN_MESSAGES_FOR_ANALYZE} OR åŸå§‹>={settings.ROOM_RAW_MIN_MESSAGES_FOR_ANALYZE}"
                        )
                        logger.info(f"[ç´¯ç§¯è§¦å‘] room={room_id}, æ— æ–°æ¶ˆæ¯ä½†ç´¯ç§¯è¾¾é˜ˆå€¼ï¼Œ{trigger_detail}")
                        expanded_context, msg_list = _get_room_history_context(db, room_id, min_msgtime=cycle_start_ms)
                        
                        if expanded_context:
                            # LLM é¢„åˆ¤æ–­ï¼ˆé«˜æ´»è·ƒç¾¤å¼ºåˆ¶è·³è¿‡é¢„åˆ¤æ–­ï¼‰
                            if settings.PRE_JUDGE_ENABLED:
                                has_issue, judge_reason = await pre_judge_has_issue(expanded_context)
                                cur_raw = state.get("raw_pending_count", 0)
                                if not has_issue and cur_raw < 30:
                                    logger.info(f"[é¢„åˆ¤æ–­è·³è¿‡] room={room_id}, åŸå› ='{judge_reason}', ç´¯ç§¯={pending_count}, raw={cur_raw}")
                                    state["pending_count"] = 0
                                    state["raw_pending_count"] = 0
                                    _update_cooldown(room_id)
                                    _save_room_state(db, room_id)
                                    continue
                                elif not has_issue:
                                    logger.info(f"[é¢„åˆ¤æ–­è¦†ç›–] room={room_id}, é¢„åˆ¤æ–­=æ— é—®é¢˜ä½†raw={cur_raw}>=30ï¼Œå¼ºåˆ¶å®Œæ•´åˆ†æ")
                            
                            # å®Œæ•´ LLM åˆ†æ
                            pre_analysis = await analyze_complete_llm(expanded_context)
                            if pre_analysis:
                                phenomenon = pre_analysis.get("phenomenon", "")
                                problem_quote = pre_analysis.get("problem_quote", "")
                                first_problem_quote = pre_analysis.get("first_problem_quote", "")
                                last_discussion_quote = pre_analysis.get("last_discussion_quote", "")
                                
                                # æ ¹æ® problem_quote æ‰¾åˆ°æœ€ä½³é”šç‚¹æ¶ˆæ¯
                                anchor_msg_id = _find_best_anchor_msg(msg_list, problem_quote)
                                if not anchor_msg_id and msg_list:
                                    mid_idx = len(msg_list) // 2
                                    anchor_msg_id = msg_list[mid_idx]["msg_id"]
                                
                                # é€šè¿‡ AI è¿”å›çš„å…³é”®å¥ç¡®å®šæ—¶é—´èŒƒå›´
                                since_msgtime = _find_msg_time_by_quote(db, room_id, msg_list, first_problem_quote)
                                until_msgtime = _find_msg_time_by_quote(db, room_id, msg_list, last_discussion_quote)
                                
                                logger.info(
                                    f"[ç´¯ç§¯è§¦å‘] room={room_id}, é—®é¢˜å…³é”®å¥='{problem_quote[:30] if problem_quote else ''}...', "
                                    f"é”šç‚¹msg_id={anchor_msg_id}, "
                                    f"æ—¶é—´èŒƒå›´: since={since_msgtime}, until={until_msgtime}"
                                )
                                
                                # å»é‡æ£€æŸ¥
                                if await _is_duplicate_issue(db, room_id, phenomenon, cycle_start_ms):
                                    logger.info(f"[å»é‡è·³è¿‡] room={room_id}, é—®é¢˜='{phenomenon[:30]}...'")
                                    state["pending_count"] = 0
                                    state["raw_pending_count"] = 0
                                    _update_cooldown(room_id)
                                    _save_room_state(db, room_id)
                                    continue
                                
                                # è°ƒç”¨ process_messageï¼ˆç´¯ç§¯è§¦å‘æ¨¡å¼ï¼‰
                                await process_message(
                                    db,
                                    msg_id=anchor_msg_id or f"accumulated_{room_id}_{int(time.time()*1000)}",
                                    room_id=room_id,
                                    sender_id="system",
                                    msg_type="text",
                                    clean_text=expanded_context,
                                    raw_text=expanded_context,
                                    sentinel=sentinel,
                                    assistant=assistant,
                                    wecom=wecom,
                                    allow_reply=False,
                                    allow_alert=True,
                                    allow_ticket=True,
                                    replay=False,
                                    since_msgtime=since_msgtime,
                                    until_msgtime=until_msgtime,
                                )
                                analyzed_count += 1
                            
                            # é‡ç½®ç´¯ç§¯è®¡æ•°ï¼ˆä»…åœ¨æˆåŠŸæå–åˆ°é—®é¢˜æ—¶é‡ç½®ï¼‰
                            state["pending_count"] = 0
                            state["raw_pending_count"] = 0
                            _update_cooldown(room_id)
                            _save_room_state(db, room_id)
                        else:
                            # LLM åˆ†æå¤±è´¥æˆ–æ— ç»“æœï¼Œä¿ç•™ pending ç­‰å¾…é‡è¯•
                            logger.warning(f"[ç´¯ç§¯è§¦å‘] room={room_id} LLMåˆ†ææ— ç»“æœï¼Œä¿ç•™pendingç­‰å¾…é‡è¯•")
                            _update_cooldown(room_id)
                            _save_room_state(db, room_id)
                    continue
                
                # ========== æƒ…å†µBï¼šæœ‰æ–°æ¶ˆæ¯ï¼Œæ­£å¸¸å¤„ç† ==========
                # 6. å…¥åº“å¹¶æ›´æ–°ç´¯ç§¯è®¡æ•°
                stored_messages = []
                raw_text_count = 0  # æ‰€æœ‰æˆåŠŸå…¥åº“çš„ text æ¶ˆæ¯è®¡æ•°ï¼ˆå«å™ªéŸ³ï¼‰
                max_msgtime_in_room = state["last_msgtime"]
                
                for r in records:
                    content = _extract_content(r.msgData)
                    if not content:
                        continue
                    
                    clean_text = DataCleanService.sanitize(content)
                    max_msgtime_in_room = max(max_msgtime_in_room, int(r.msgtime))
                    
                    msg_id = str(r.msgid)
                    exists = (
                        db.query(WeComMessage)
                        .filter(WeComMessage.msg_id == msg_id)
                        .first()
                    )
                    if exists:
                        continue
                    
                    # å°† ChatRecord.msgtimeï¼ˆæ¯«ç§’æ—¶é—´æˆ³ï¼‰è½¬æ¢ä¸º datetime
                    msg_datetime = datetime.fromtimestamp(int(r.msgtime) / 1000) if r.msgtime else None
                    
                    record = WeComMessage(
                        msg_id=msg_id,
                        seq=int(r.seq or 0),
                        room_id=room_id,
                        sender_id=str(r.sender or ""),
                        msg_type=str(r.msgtype),
                        content_raw=content,
                        content_clean=clean_text,
                        msg_time=msg_datetime,  # ä½¿ç”¨æ¶ˆæ¯å®é™…å‘é€æ—¶é—´
                        is_noise=DataCleanService.is_noise(clean_text),
                    )
                    db.add(record)
                    try:
                        db.commit()
                    except IntegrityError:
                        db.rollback()
                        continue
                    
                    # æ‰€æœ‰æˆåŠŸå…¥åº“çš„ text æ¶ˆæ¯éƒ½è®¡å…¥åŸå§‹è®¡æ•°
                    if str(r.msgtype) == "text":
                        raw_text_count += 1
                    
                    # æ”¶é›†éå™ªéŸ³æ–‡æœ¬æ¶ˆæ¯
                    if not DataCleanService.is_noise(clean_text) and str(r.msgtype) == "text":
                        stored_messages.append({
                            "msg_id": msg_id,
                            "clean_text": clean_text,
                            "content_raw": content,
                            "record": r,
                        })
                        vector_messages.append({
                            "msg_id": msg_id,
                            "room_id": room_id,
                            "sender_id": str(r.sender or ""),
                            "content_raw": content,
                            "content_clean": clean_text,
                            "msg_time": msg_datetime,  # ä½¿ç”¨æ¶ˆæ¯å®é™…å‘é€æ—¶é—´
                        })
                
                # æ›´æ–°ç¾¤èŠçŠ¶æ€ï¼ˆåŒè®¡æ•°ï¼šæœ‰æ•ˆæ¶ˆæ¯ + åŸå§‹æ¶ˆæ¯ï¼‰
                state["last_msgtime"] = max_msgtime_in_room
                state["pending_count"] += len(stored_messages)
                state["raw_pending_count"] = state.get("raw_pending_count", 0) + raw_text_count
                state["last_processed_at"] = time.time()
                
                # æŒä¹…åŒ–çŠ¶æ€åˆ°æ•°æ®åº“
                _save_room_state(db, room_id)
                
                # 7. åˆ¤æ–­æ˜¯å¦è§¦å‘åˆ†æï¼ˆåŸºäºåŒé˜ˆå€¼ï¼šæœ‰æ•ˆæ¶ˆæ¯æ•° OR åŸå§‹æ¶ˆæ¯æ•°ï¼‰
                pending_count = state["pending_count"]
                raw_pending = state.get("raw_pending_count", 0)
                in_cooldown = _is_in_cooldown(room_id)
                
                # å¦‚æœæ²¡æœ‰æ–°æ¶ˆæ¯ä¸”åŒé˜ˆå€¼å‡ä¸æ»¡è¶³ï¼Œè·³è¿‡
                if not stored_messages and (
                    pending_count < settings.ROOM_MIN_MESSAGES_FOR_ANALYZE
                    and raw_pending < settings.ROOM_RAW_MIN_MESSAGES_FOR_ANALYZE
                ):
                    continue
                
                # è·å–ä¸Šä¸‹æ–‡ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬è½®æ¶ˆæ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»å†å²è·å–ï¼‰
                if stored_messages:
                    chat_context = _merge_chat_context(records)
                else:
                    # æ²¡æœ‰æ–°æ¶ˆæ¯ä½†ç´¯ç§¯æ•°è¾¾åˆ°é˜ˆå€¼ï¼Œä»å†å²è®°å½•è·å–ä¸Šä¸‹æ–‡
                    expanded_context, _ = _get_room_history_context(db, room_id, min_msgtime=cycle_start_ms)
                    chat_context = expanded_context or ""
                
                has_high_risk = _contains_high_risk_keyword(chat_context)
                
                should_analyze = False
                trigger_reason = ""
                
                # é«˜é£é™©å…³é”®è¯ç»•è¿‡å†·å´ï¼Œä½†ä»éœ€è¾¾åˆ°æœ€å°‘æ¶ˆæ¯æ•°
                if has_high_risk and pending_count >= settings.HIGH_RISK_MIN_MESSAGES:
                    should_analyze = True
                    trigger_reason = f"é«˜é£é™©å…³é”®è¯(æœ‰æ•ˆ={pending_count}, åŸå§‹={raw_pending})"
                elif not in_cooldown and pending_count >= settings.ROOM_MIN_MESSAGES_FOR_ANALYZE:
                    should_analyze = True
                    trigger_reason = f"æœ‰æ•ˆæ¶ˆæ¯æ•°={pending_count}>=é˜ˆå€¼{settings.ROOM_MIN_MESSAGES_FOR_ANALYZE}"
                elif not in_cooldown and raw_pending >= settings.ROOM_RAW_MIN_MESSAGES_FOR_ANALYZE:
                    should_analyze = True
                    trigger_reason = f"åŸå§‹æ¶ˆæ¯æ•°={raw_pending}>=åŸå§‹é˜ˆå€¼{settings.ROOM_RAW_MIN_MESSAGES_FOR_ANALYZE}"
                
                # 8. è§¦å‘åˆ†æ
                if should_analyze:
                    # è·å–æ›´ä¸°å¯Œçš„å†å²ä¸Šä¸‹æ–‡ï¼ˆåŒæ—¶è¿”å›æ¶ˆæ¯åˆ—è¡¨ç”¨äºåŒ¹é…é—®é¢˜ä½ç½®ï¼‰
                    expanded_context, msg_list = _get_room_history_context(db, room_id, min_msgtime=cycle_start_ms)
                    if not expanded_context:
                        expanded_context = chat_context
                        msg_list = [{"msg_id": m["msg_id"], "content": m["clean_text"]} for m in stored_messages]
                    
                    # ========== LLM é¢„åˆ¤æ–­ï¼ˆè½»é‡çº§ï¼ŒèŠ‚çœ tokensï¼‰ ==========
                    # åœ¨å®Œæ•´åˆ†æå‰å…ˆåˆ¤æ–­å¯¹è¯æ˜¯å¦åŒ…å«æœ‰æ•ˆé—®é¢˜ï¼ˆé«˜æ´»è·ƒç¾¤å¼ºåˆ¶è·³è¿‡é¢„åˆ¤æ–­ï¼‰
                    if settings.PRE_JUDGE_ENABLED:
                        has_issue, judge_reason = await pre_judge_has_issue(expanded_context)
                        cur_raw = state.get("raw_pending_count", 0)
                        if not has_issue and cur_raw < 30:
                            logger.info(
                                f"[é¢„åˆ¤æ–­è·³è¿‡] room={room_id}, åŸå› ='{judge_reason}', "
                                f"ç´¯ç§¯æ¶ˆæ¯={pending_count}, raw={cur_raw}, è·³è¿‡å®Œæ•´åˆ†æ"
                            )
                            # é‡ç½®ç´¯ç§¯è®¡æ•°ä½†ä¸è§¦å‘å®Œæ•´åˆ†æ
                            state["pending_count"] = 0
                            state["raw_pending_count"] = 0
                            _update_cooldown(room_id)
                            _save_room_state(db, room_id)  # æŒä¹…åŒ–çŠ¶æ€
                            continue
                        elif not has_issue:
                            logger.info(f"[é¢„åˆ¤æ–­è¦†ç›–] room={room_id}, é¢„åˆ¤æ–­=æ— é—®é¢˜ä½†raw={cur_raw}>=30ï¼Œå¼ºåˆ¶å®Œæ•´åˆ†æ")
                        else:
                            logger.debug(f"[é¢„åˆ¤æ–­é€šè¿‡] room={room_id}, åŸå› ='{judge_reason}'")
                    
                    # å…ˆè°ƒç”¨ LLM è·å– problem_quoteï¼ˆé—®é¢˜åŸæ–‡å…³é”®å¥ï¼‰ï¼Œç”¨äºå®šä½é—®é¢˜æ¶ˆæ¯
                    pre_analysis = await analyze_complete_llm(expanded_context)
                    problem_quote = pre_analysis.get("problem_quote", "")
                    phenomenon = pre_analysis.get("phenomenon", "")
                    first_problem_quote = pre_analysis.get("first_problem_quote", "")
                    last_discussion_quote = pre_analysis.get("last_discussion_quote", "")
                    
                    # ========== é—®é¢˜å»é‡æ£€æŸ¥ ==========
                    if await _is_duplicate_issue(db, room_id, phenomenon, cycle_start_ms):
                        logger.info(
                            f"[å»é‡è·³è¿‡] room={room_id}, é—®é¢˜='{phenomenon[:30]}...' "
                            f"åœ¨å½“å‰å‘¨æœŸå†…å·²æœ‰ç›¸ä¼¼å·¥å•ï¼Œè·³è¿‡å»ºå•"
                        )
                        state["pending_count"] = 0
                        state["raw_pending_count"] = 0
                        _update_cooldown(room_id)
                        _save_room_state(db, room_id)
                        continue
                    
                    # æ ¹æ® problem_quote åœ¨æ¶ˆæ¯åˆ—è¡¨ä¸­åŒ¹é…æœ€ç›¸å…³çš„æ¶ˆæ¯ä½œä¸ºé”šç‚¹
                    anchor_msg_id = _find_best_anchor_msg(msg_list, problem_quote)
                    if not anchor_msg_id and msg_list:
                        anchor_msg_id = msg_list[0]["msg_id"]
                    
                    # é€šè¿‡ AI è¿”å›çš„å…³é”®å¥ç¡®å®šæ—¶é—´èŒƒå›´
                    since_msgtime = _find_msg_time_by_quote(db, room_id, msg_list, first_problem_quote)
                    until_msgtime = _find_msg_time_by_quote(db, room_id, msg_list, last_discussion_quote)
                    
                    logger.info(
                        f"[ç¾¤èŠè½®è¯¢] room={room_id} è§¦å‘LLMåˆ†æ, "
                        f"åŸå› ={trigger_reason}, æœ¬æ¬¡å…¥åº“={len(stored_messages)}, "
                        f"ä¸Šä¸‹æ–‡é•¿åº¦={len(expanded_context)}å­—ç¬¦, "
                        f"é—®é¢˜å…³é”®å¥='{problem_quote[:30] if problem_quote else ''}...', é”šç‚¹msg_id={anchor_msg_id}, "
                        f"æ—¶é—´èŒƒå›´: since={since_msgtime}, until={until_msgtime}"
                    )
                    
                    # è·å–æ¶ˆæ¯ä¿¡æ¯ï¼ˆä¼˜å…ˆç”¨æœ¬è½®å…¥åº“çš„ï¼Œå¦åˆ™ä»å†å²ä¸Šä¸‹æ–‡è·å–ï¼‰
                    if stored_messages:
                        last_msg = stored_messages[-1]
                        fallback_msg_id = last_msg["msg_id"]
                        sender_id = str(last_msg["record"].sender or "")
                        msg_type = str(last_msg["record"].msgtype)
                    else:
                        fallback_msg_id = msg_list[-1]["msg_id"] if msg_list else f"reset_{room_id}_{int(time.time())}"
                        sender_id = ""
                        msg_type = "text"
                    
                    await process_message(
                        db,
                        msg_id=anchor_msg_id or fallback_msg_id,
                        room_id=room_id,
                        sender_id=sender_id,
                        msg_type=msg_type,
                        clean_text=expanded_context,
                        raw_text=expanded_context,
                        sentinel=sentinel,
                        assistant=assistant,
                        wecom=wecom,
                        allow_reply=True,
                        allow_alert=True,
                        allow_ticket=True,
                        replay=False,
                        since_msgtime=since_msgtime,
                        until_msgtime=until_msgtime,
                    )
                    
                    # åˆ†æåé‡ç½®ç´¯ç§¯è®¡æ•°å¹¶æ›´æ–°å†·å´
                    state["pending_count"] = 0
                    state["raw_pending_count"] = 0
                    _update_cooldown(room_id)
                    _save_room_state(db, room_id)  # æŒä¹…åŒ–çŠ¶æ€
                    analyzed_count += 1
                else:
                    logger.debug(
                        f"[ç¾¤èŠè½®è¯¢] room={room_id}, æœ¬æ¬¡å…¥åº“={len(stored_messages)}, "
                        f"æœ‰æ•ˆç´¯ç§¯={pending_count}, åŸå§‹ç´¯ç§¯={raw_pending}, "
                        f"å†·å´ä¸­={in_cooldown}, é«˜é£é™©={has_high_risk}"
                    )
            
            # FAQ è‡ªåŠ¨ç”Ÿæˆï¼ˆæ¯è½®æœ«å°¾æ‰§è¡Œä¸€æ¬¡ï¼‰
            if analyzed_count > 0 and settings.AUTO_FAQ_ENABLED:
                issues = (
                    db.query(Issue)
                    .order_by(Issue.created_at.desc())
                    .limit(200)
                    .all()
                )
                faq_items = await faq_service.generate_from_issues(
                    issues,
                    min_group=settings.AUTO_FAQ_MIN_GROUP,
                    max_groups=settings.AUTO_FAQ_MAX_GROUPS,
                )
                if faq_items:
                    for item in faq_items:
                        db.add(item)
                    db.commit()
            
            # å‘é‡åŒ–å­˜å‚¨
            if vector_messages:
                vector_kb.add_wecom_messages(vector_messages)
                
        finally:
            db.close()

        await asyncio.sleep(settings.POLLING_INTERVAL_SECONDS)


async def run_end_of_cycle_analysis() -> dict:
    """
    å‘¨æœŸç»“æŸå…œåº•åˆ†æï¼šå¯¹æœªè¾¾æ­£å¸¸é˜ˆå€¼ä½†æœ‰ç´¯ç§¯æ¶ˆæ¯çš„ç¾¤èŠè¿›è¡Œåˆ†æ
    
    åœ¨æ¯æ—¥å‘¨æœŸç»“æŸå‰ï¼ˆé»˜è®¤ 8:30ï¼‰æ‰§è¡Œï¼Œç¡®ä¿ä½æ´»è·ƒç¾¤èŠçš„é—®é¢˜ä¸è¢«é—æ¼ã€‚
    
    å¤„ç†èŒƒå›´ï¼š
    - pending_count >= END_OF_CYCLE_MIN_MESSAGESï¼ˆé»˜è®¤ 3ï¼‰
    - pending_count < ROOM_MIN_MESSAGES_FOR_ANALYZEï¼ˆé»˜è®¤ 10ï¼‰
    
    Returns:
        dict: åŒ…å«åˆ†æç»“æœç»Ÿè®¡
            - total_rooms: æ»¡è¶³æ¡ä»¶çš„ç¾¤èŠæ•°
            - analyzed_count: æˆåŠŸåˆ†æçš„ç¾¤èŠæ•°
            - skipped_pre_judge: é¢„åˆ¤æ–­è·³è¿‡çš„ç¾¤èŠæ•°
            - skipped_duplicate: å»é‡è·³è¿‡çš„ç¾¤èŠæ•°
            - skipped_no_issue: æ— æœ‰æ•ˆé—®é¢˜çš„ç¾¤èŠæ•°
    """
    logger.info(
        f"[å…œåº•åˆ†æ] å¼€å§‹æ‰§è¡Œå‘¨æœŸç»“æŸå…œåº•åˆ†æï¼Œ"
        f"é˜ˆå€¼èŒƒå›´=[{settings.END_OF_CYCLE_MIN_MESSAGES}, {settings.ROOM_MIN_MESSAGES_FOR_ANALYZE})"
    )
    
    result = {
        "total_rooms": 0,
        "analyzed_count": 0,
        "skipped_pre_judge": 0,
        "skipped_duplicate": 0,
        "skipped_no_issue": 0,
    }
    
    db = SessionLocal()
    try:
        # åˆå§‹åŒ– agentsï¼ˆä¸ polling_loop ç±»ä¼¼ï¼‰
        sentinel = SentinelAgent()
        assistant = AssistantAgent()
        wecom = WeComService()
        
        cycle_start_ms = _get_current_cycle_start()
        
        # ç­›é€‰æ»¡è¶³æ¡ä»¶çš„ç¾¤èŠ
        qualifying_rooms = []
        for room_id, state in _room_state.items():
            pending_count = state.get("pending_count", 0)
            if (
                pending_count >= settings.END_OF_CYCLE_MIN_MESSAGES
                and pending_count < settings.ROOM_MIN_MESSAGES_FOR_ANALYZE
            ):
                qualifying_rooms.append((room_id, pending_count))
        
        result["total_rooms"] = len(qualifying_rooms)
        
        if not qualifying_rooms:
            logger.info("[å…œåº•åˆ†æ] æ— æ»¡è¶³æ¡ä»¶çš„ç¾¤èŠï¼Œè·³è¿‡åˆ†æ")
            return result
        
        logger.info(f"[å…œåº•åˆ†æ] æ‰¾åˆ° {len(qualifying_rooms)} ä¸ªç¾¤èŠéœ€è¦å…œåº•åˆ†æ")
        
        for room_id, pending_count in qualifying_rooms:
            state = _get_room_state(room_id)
            
            logger.info(f"[å…œåº•åˆ†æ] å¤„ç†ç¾¤èŠ room={room_id}, pending_count={pending_count}")
            
            # è·å–å†å²ä¸Šä¸‹æ–‡
            expanded_context, msg_list = _get_room_history_context(db, room_id, min_msgtime=cycle_start_ms)
            
            if not expanded_context:
                logger.warning(f"[å…œåº•åˆ†æ] room={room_id} æ— æ³•è·å–å†å²ä¸Šä¸‹æ–‡ï¼Œè·³è¿‡")
                result["skipped_no_issue"] += 1
                continue
            
            # LLM é¢„åˆ¤æ–­ï¼ˆé«˜æ´»è·ƒç¾¤å¼ºåˆ¶è·³è¿‡é¢„åˆ¤æ–­ï¼‰
            if settings.PRE_JUDGE_ENABLED:
                has_issue, judge_reason = await pre_judge_has_issue(expanded_context)
                cur_raw = state.get("raw_pending_count", 0)
                if not has_issue and cur_raw < 30:
                    logger.info(f"[å…œåº•åˆ†æ] room={room_id} é¢„åˆ¤æ–­è·³è¿‡, åŸå› ='{judge_reason}', raw={cur_raw}")
                    state["pending_count"] = 0
                    state["raw_pending_count"] = 0
                    _save_room_state(db, room_id)
                    result["skipped_pre_judge"] += 1
                    continue
                elif not has_issue:
                    logger.info(f"[å…œåº•åˆ†æ] room={room_id} é¢„åˆ¤æ–­è¦†ç›–, raw={cur_raw}>=30ï¼Œå¼ºåˆ¶åˆ†æ")
            
            # å®Œæ•´ LLM åˆ†æ
            pre_analysis = await analyze_complete_llm(expanded_context)
            if not pre_analysis:
                logger.warning(f"[å…œåº•åˆ†æ] room={room_id} LLMåˆ†æè¿”å›ç©ºï¼Œä¿ç•™pendingç­‰å¾…ä¸‹æ¬¡é‡è¯•")
                _update_cooldown(room_id)  # è¿›å…¥å†·å´é¿å…é¢‘ç¹è°ƒç”¨
                # ä¸æ¸…é›¶ pending_count å’Œ raw_pending_countï¼Œä¸‹æ¬¡å†·å´ç»“æŸåé‡æ–°å°è¯•
                _save_room_state(db, room_id)
                result["skipped_no_issue"] += 1
                continue
            
            phenomenon = pre_analysis.get("phenomenon", "")
            problem_quote = pre_analysis.get("problem_quote", "")
            first_problem_quote = pre_analysis.get("first_problem_quote", "")
            last_discussion_quote = pre_analysis.get("last_discussion_quote", "")
            
            # å»é‡æ£€æŸ¥
            if await _is_duplicate_issue(db, room_id, phenomenon, cycle_start_ms):
                logger.info(f"[å…œåº•åˆ†æ] room={room_id} å»é‡è·³è¿‡, é—®é¢˜='{phenomenon[:30]}...'")
                state["pending_count"] = 0
                state["raw_pending_count"] = 0
                _save_room_state(db, room_id)
                result["skipped_duplicate"] += 1
                continue
            
            # æ‰¾åˆ°æœ€ä½³é”šç‚¹æ¶ˆæ¯
            anchor_msg_id = _find_best_anchor_msg(msg_list, problem_quote)
            if not anchor_msg_id and msg_list:
                mid_idx = len(msg_list) // 2
                anchor_msg_id = msg_list[mid_idx]["msg_id"]
            
            # é€šè¿‡ AI è¿”å›çš„å…³é”®å¥ç¡®å®šæ—¶é—´èŒƒå›´
            since_msgtime = _find_msg_time_by_quote(db, room_id, msg_list, first_problem_quote)
            until_msgtime = _find_msg_time_by_quote(db, room_id, msg_list, last_discussion_quote)
            
            logger.info(
                f"[å…œåº•åˆ†æ] room={room_id} è§¦å‘åˆ†æ, "
                f"é—®é¢˜='{phenomenon[:30]}...', é”šç‚¹msg_id={anchor_msg_id}"
            )
            
            # è°ƒç”¨ process_message è¿›è¡Œå»ºå•å’Œæ¨é€
            await process_message(
                db,
                msg_id=anchor_msg_id or f"end_of_cycle_{room_id}_{int(time.time()*1000)}",
                room_id=room_id,
                sender_id="system",
                msg_type="text",
                clean_text=expanded_context,
                raw_text=expanded_context,
                sentinel=sentinel,
                assistant=assistant,
                wecom=wecom,
                allow_reply=False,  # å…œåº•åˆ†æä¸è‡ªåŠ¨å›å¤
                allow_alert=True,
                allow_ticket=True,
                replay=False,
                since_msgtime=since_msgtime,
                until_msgtime=until_msgtime,
            )
            
            # é‡ç½®ç´¯ç§¯è®¡æ•°å¹¶æŒä¹…åŒ–
            state["pending_count"] = 0
            state["raw_pending_count"] = 0
            _update_cooldown(room_id)
            _save_room_state(db, room_id)
            result["analyzed_count"] += 1
        
        logger.info(
            f"[å…œåº•åˆ†æ] å®Œæˆï¼Œæ€»è®¡={result['total_rooms']}, "
            f"åˆ†æ={result['analyzed_count']}, é¢„åˆ¤æ–­è·³è¿‡={result['skipped_pre_judge']}, "
            f"å»é‡è·³è¿‡={result['skipped_duplicate']}, æ— æœ‰æ•ˆé—®é¢˜={result['skipped_no_issue']}"
        )
        
    except Exception as e:
        logger.error(f"[å…œåº•åˆ†æ] æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
    finally:
        db.close()
    
    return result
